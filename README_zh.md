# DiffSynth-Studio

<a href="https://github.com/modelscope/DiffSynth-Studio"><img src=".github/workflows/logo.gif" title="Logo" style="max-width:100%;" width="55" /></a> <a href="https://trendshift.io/repositories/10946" target="_blank"><img src="https://trendshift.io/api/badge/repositories/10946" alt="modelscope%2FDiffSynth-Studio | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a></p>

[![PyPI](https://img.shields.io/pypi/v/DiffSynth)](https://pypi.org/project/DiffSynth/)
[![license](https://img.shields.io/github/license/modelscope/DiffSynth-Studio.svg)](https://github.com/modelscope/DiffSynth-Studio/blob/master/LICENSE)
[![open issues](https://isitmaintained.com/badge/open/modelscope/DiffSynth-Studio.svg)](https://github.com/modelscope/DiffSynth-Studio/issues)
[![GitHub pull-requests](https://img.shields.io/github/issues-pr/modelscope/DiffSynth-Studio.svg)](https://GitHub.com/modelscope/DiffSynth-Studio/pull/)
[![GitHub latest commit](https://badgen.net/github/last-commit/modelscope/DiffSynth-Studio)](https://GitHub.com/modelscope/DiffSynth-Studio/commit/) 

[Switch to English](./README.md)

## ç®€ä»‹

æ¬¢è¿æ¥åˆ° Diffusion æ¨¡å‹çš„é­”æ³•ä¸–ç•Œï¼DiffSynth-Studio æ˜¯ç”±[é­”æ­ç¤¾åŒº](https://www.modelscope.cn/)å›¢é˜Ÿå¼€å‘å’Œç»´æŠ¤çš„å¼€æº Diffusion æ¨¡å‹å¼•æ“ã€‚æˆ‘ä»¬æœŸæœ›ä»¥æ¡†æ¶å»ºè®¾å­µåŒ–æŠ€æœ¯åˆ›æ–°ï¼Œå‡èšå¼€æºç¤¾åŒºçš„åŠ›é‡ï¼Œæ¢ç´¢ç”Ÿæˆå¼æ¨¡å‹æŠ€æœ¯çš„è¾¹ç•Œï¼

DiffSynth ç›®å‰åŒ…æ‹¬ä¸¤ä¸ªå¼€æºé¡¹ç›®ï¼š
* [DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio): èšç„¦äºæ¿€è¿›çš„æŠ€æœ¯æ¢ç´¢ï¼Œé¢å‘å­¦æœ¯ç•Œï¼Œæä¾›æ›´å‰æ²¿çš„æ¨¡å‹èƒ½åŠ›æ”¯æŒã€‚
* [DiffSynth-Engine](https://github.com/modelscope/DiffSynth-Engine): èšç„¦äºç¨³å®šçš„æ¨¡å‹éƒ¨ç½²ï¼Œé¢å‘å·¥ä¸šç•Œï¼Œæä¾›æ›´é«˜çš„è®¡ç®—æ€§èƒ½ä¸æ›´ç¨³å®šçš„åŠŸèƒ½ã€‚

[DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio) ä¸ [DiffSynth-Engine](https://github.com/modelscope/DiffSynth-Engine) ä½œä¸ºé­”æ­ç¤¾åŒº [AIGC ä¸“åŒº](https://modelscope.cn/aigc/home) çš„æ ¸å¿ƒæŠ€æœ¯æ”¯æ’‘ï¼Œæä¾›äº†å¼ºå¤§çš„AIç”Ÿæˆå†…å®¹èƒ½åŠ›ã€‚æ¬¢è¿ä½“éªŒæˆ‘ä»¬ç²¾å¿ƒæ‰“é€ çš„äº§å“åŒ–åŠŸèƒ½ï¼Œå¼€å¯æ‚¨çš„AIåˆ›ä½œä¹‹æ—…ï¼

## å®‰è£…

ä»æºç å®‰è£…ï¼ˆæ¨èï¼‰ï¼š

```
git clone https://github.com/modelscope/DiffSynth-Studio.git
cd DiffSynth-Studio
pip install -e .
```

<details>
<summary>å…¶ä»–å®‰è£…æ–¹å¼</summary>

ä» pypi å®‰è£…ï¼ˆå­˜åœ¨ç‰ˆæœ¬æ›´æ–°å»¶è¿Ÿï¼Œå¦‚éœ€ä½¿ç”¨æœ€æ–°åŠŸèƒ½ï¼Œè¯·ä»æºç å®‰è£…ï¼‰

```
pip install diffsynth
```

å¦‚æœåœ¨å®‰è£…è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œå¯èƒ½æ˜¯ç”±ä¸Šæ¸¸ä¾èµ–åŒ…å¯¼è‡´çš„ï¼Œè¯·å‚è€ƒè¿™äº›åŒ…çš„æ–‡æ¡£ï¼š

* [torch](https://pytorch.org/get-started/locally/)
* [sentencepiece](https://github.com/google/sentencepiece)
* [cmake](https://cmake.org)
* [cupy](https://docs.cupy.dev/en/stable/install.html)

</details>



## åŸºç¡€æ¡†æ¶

DiffSynth-Studio ä¸ºä¸»æµ Diffusion æ¨¡å‹ï¼ˆåŒ…æ‹¬ FLUXã€Wan ç­‰ï¼‰é‡æ–°è®¾è®¡äº†æ¨ç†å’Œè®­ç»ƒæµæ°´çº¿ï¼Œèƒ½å¤Ÿå®ç°é«˜æ•ˆçš„æ˜¾å­˜ç®¡ç†ã€çµæ´»çš„æ¨¡å‹è®­ç»ƒã€‚

### FLUX ç³»åˆ—

è¯¦ç»†é¡µé¢ï¼š[./examples/flux/](./examples/flux/)

![Image](https://github.com/user-attachments/assets/c01258e2-f251-441a-aa1e-ebb22f02594d)

<details>

<summary>å¿«é€Ÿå¼€å§‹</summary>

```python
import torch
from diffsynth.pipelines.flux_image_new import FluxImagePipeline, ModelConfig

pipe = FluxImagePipeline.from_pretrained(
    torch_dtype=torch.bfloat16,
    device="cuda",
    model_configs=[
        ModelConfig(model_id="black-forest-labs/FLUX.1-dev", origin_file_pattern="flux1-dev.safetensors"),
        ModelConfig(model_id="black-forest-labs/FLUX.1-dev", origin_file_pattern="text_encoder/model.safetensors"),
        ModelConfig(model_id="black-forest-labs/FLUX.1-dev", origin_file_pattern="text_encoder_2/"),
        ModelConfig(model_id="black-forest-labs/FLUX.1-dev", origin_file_pattern="ae.safetensors"),
    ],
)

image = pipe(prompt="a cat", seed=0)
image.save("image.jpg")
```

</details>

<details>

<summary>æ¨¡å‹æ€»è§ˆ</summary>

|æ¨¡å‹ ID|é¢å¤–å‚æ•°|æ¨ç†|ä½æ˜¾å­˜æ¨ç†|å…¨é‡è®­ç»ƒ|å…¨é‡è®­ç»ƒåéªŒè¯|LoRA è®­ç»ƒ|LoRA è®­ç»ƒåéªŒè¯|
|-|-|-|-|-|-|-|-|
|[FLUX.1-dev](https://www.modelscope.cn/models/black-forest-labs/FLUX.1-dev)||[code](./examples/flux/model_inference/FLUX.1-dev.py)|[code](./examples/flux/model_inference_low_vram/FLUX.1-dev.py)|[code](./examples/flux/model_training/full/FLUX.1-dev.sh)|[code](./examples/flux/model_training/validate_full/FLUX.1-dev.py)|[code](./examples/flux/model_training/lora/FLUX.1-dev.sh)|[code](./examples/flux/model_training/validate_lora/FLUX.1-dev.py)|
|[FLUX.1-Kontext-dev](https://www.modelscope.cn/models/black-forest-labs/FLUX.1-Kontext-dev)|`kontext_images`|[code](./examples/flux/model_inference/FLUX.1-Kontext-dev.py)|[code](./examples/flux/model_inference_low_vram/FLUX.1-Kontext-dev.py)|[code](./examples/flux/model_training/full/FLUX.1-Kontext-dev.sh)|[code](./examples/flux/model_training/validate_full/FLUX.1-Kontext-dev.py)|[code](./examples/flux/model_training/lora/FLUX.1-Kontext-dev.sh)|[code](./examples/flux/model_training/validate_lora/FLUX.1-Kontext-dev.py)|
|[FLUX.1-dev-Controlnet-Inpainting-Beta](https://www.modelscope.cn/models/alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Beta)|`controlnet_inputs`|[code](./examples/flux/model_inference/FLUX.1-dev-Controlnet-Inpainting-Beta.py)|[code](./examples/flux/model_inference_low_vram/FLUX.1-dev-Controlnet-Inpainting-Beta.py)|[code](./examples/flux/model_training/full/FLUX.1-dev-Controlnet-Inpainting-Beta.sh)|[code](./examples/flux/model_training/validate_full/FLUX.1-dev-Controlnet-Inpainting-Beta.py)|[code](./examples/flux/model_training/lora/FLUX.1-dev-Controlnet-Inpainting-Beta.sh)|[code](./examples/flux/model_training/validate_lora/FLUX.1-dev-Controlnet-Inpainting-Beta.py)|
|[FLUX.1-dev-Controlnet-Union-alpha](https://www.modelscope.cn/models/InstantX/FLUX.1-dev-Controlnet-Union-alpha)|`controlnet_inputs`|[code](./examples/flux/model_inference/FLUX.1-dev-Controlnet-Union-alpha.py)|[code](./examples/flux/model_inference_low_vram/FLUX.1-dev-Controlnet-Union-alpha.py)|[code](./examples/flux/model_training/full/FLUX.1-dev-Controlnet-Union-alpha.sh)|[code](./examples/flux/model_training/validate_full/FLUX.1-dev-Controlnet-Union-alpha.py)|[code](./examples/flux/model_training/lora/FLUX.1-dev-Controlnet-Union-alpha.sh)|[code](./examples/flux/model_training/validate_lora/FLUX.1-dev-Controlnet-Union-alpha.py)|
|[FLUX.1-dev-Controlnet-Upscaler](https://www.modelscope.cn/models/jasperai/Flux.1-dev-Controlnet-Upscaler)|`controlnet_inputs`|[code](./examples/flux/model_inference/FLUX.1-dev-Controlnet-Upscaler.py)|[code](./examples/flux/model_inference_low_vram/FLUX.1-dev-Controlnet-Upscaler.py)|[code](./examples/flux/model_training/full/FLUX.1-dev-Controlnet-Upscaler.sh)|[code](./examples/flux/model_training/validate_full/FLUX.1-dev-Controlnet-Upscaler.py)|[code](./examples/flux/model_training/lora/FLUX.1-dev-Controlnet-Upscaler.sh)|[code](./examples/flux/model_training/validate_lora/FLUX.1-dev-Controlnet-Upscaler.py)|
|[FLUX.1-dev-IP-Adapter](https://www.modelscope.cn/models/InstantX/FLUX.1-dev-IP-Adapter)|`ipadapter_images`, `ipadapter_scale`|[code](./examples/flux/model_inference/FLUX.1-dev-IP-Adapter.py)|[code](./examples/flux/model_inference_low_vram/FLUX.1-dev-IP-Adapter.py)|[code](./examples/flux/model_training/full/FLUX.1-dev-IP-Adapter.sh)|[code](./examples/flux/model_training/validate_full/FLUX.1-dev-IP-Adapter.py)|[code](./examples/flux/model_training/lora/FLUX.1-dev-IP-Adapter.sh)|[code](./examples/flux/model_training/validate_lora/FLUX.1-dev-IP-Adapter.py)|
|[FLUX.1-dev-InfiniteYou](https://www.modelscope.cn/models/ByteDance/InfiniteYou)|`infinityou_id_image`, `infinityou_guidance`, `controlnet_inputs`|[code](./examples/flux/model_inference/FLUX.1-dev-InfiniteYou.py)|[code](./examples/flux/model_inference_low_vram/FLUX.1-dev-InfiniteYou.py)|[code](./examples/flux/model_training/full/FLUX.1-dev-InfiniteYou.sh)|[code](./examples/flux/model_training/validate_full/FLUX.1-dev-InfiniteYou.py)|[code](./examples/flux/model_training/lora/FLUX.1-dev-InfiniteYou.sh)|[code](./examples/flux/model_training/validate_lora/FLUX.1-dev-InfiniteYou.py)|
|[FLUX.1-dev-EliGen](https://www.modelscope.cn/models/DiffSynth-Studio/Eligen)|`eligen_entity_prompts`, `eligen_entity_masks`, `eligen_enable_on_negative`, `eligen_enable_inpaint`|[code](./examples/flux/model_inference/FLUX.1-dev-EliGen.py)|[code](./examples/flux/model_inference_low_vram/FLUX.1-dev-EliGen.py)|-|-|[code](./examples/flux/model_training/lora/FLUX.1-dev-EliGen.sh)|[code](./examples/flux/model_training/validate_lora/FLUX.1-dev-EliGen.py)|
|[FLUX.1-dev-LoRA-Encoder](https://www.modelscope.cn/models/DiffSynth-Studio/LoRA-Encoder-FLUX.1-Dev)|`lora_encoder_inputs`, `lora_encoder_scale`|[code](./examples/flux/model_inference/FLUX.1-dev-LoRA-Encoder.py)|[code](./examples/flux/model_inference_low_vram/FLUX.1-dev-LoRA-Encoder.py)|[code](./examples/flux/model_training/full/FLUX.1-dev-LoRA-Encoder.sh)|[code](./examples/flux/model_training/validate_full/FLUX.1-dev-LoRA-Encoder.py)|-|-|
|[FLUX.1-dev-LoRA-Fusion-Preview](https://modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev)||[code](./examples/flux/model_inference/FLUX.1-dev-LoRA-Fusion.py)|-|-|-|-|-|
|[Step1X-Edit](https://www.modelscope.cn/models/stepfun-ai/Step1X-Edit)|`step1x_reference_image`|[code](./examples/flux/model_inference/Step1X-Edit.py)|[code](./examples/flux/model_inference_low_vram/Step1X-Edit.py)|[code](./examples/flux/model_training/full/Step1X-Edit.sh)|[code](./examples/flux/model_training/validate_full/Step1X-Edit.py)|[code](./examples/flux/model_training/lora/Step1X-Edit.sh)|[code](./examples/flux/model_training/validate_lora/Step1X-Edit.py)|
|[FLEX.2-preview](https://www.modelscope.cn/models/ostris/Flex.2-preview)|`flex_inpaint_image`, `flex_inpaint_mask`, `flex_control_image`, `flex_control_strength`, `flex_control_stop`|[code](./examples/flux/model_inference/FLEX.2-preview.py)|[code](./examples/flux/model_inference_low_vram/FLEX.2-preview.py)|[code](./examples/flux/model_training/full/FLEX.2-preview.sh)|[code](./examples/flux/model_training/validate_full/FLEX.2-preview.py)|[code](./examples/flux/model_training/lora/FLEX.2-preview.sh)|[code](./examples/flux/model_training/validate_lora/FLEX.2-preview.py)|
|[Nexus-Gen-Edit](https://www.modelscope.cn/models/DiffSynth-Studio/Nexus-GenV2)|`nexus_gen_reference_image`|[code](./examples/flux/model_inference/Nexus-Gen-Editing.py)|[code](./examples/flux/model_inference_low_vram/Nexus-Gen-Editing.py)|[code](./examples/flux/model_training/full/FLUX.1-NexusGen-Edit.sh)|[code](./examples/flux/model_training/validate_full/Nexus-Gen-Editing.py)|[code](./examples/flux/model_training/lora/FLUX.1-NexusGen-Edit.sh)|[code](./examples/flux/model_training/validate_lora/Nexus-Gen-Editing.py)|
</details>

### Wan ç³»åˆ—

è¯¦ç»†é¡µé¢ï¼š[./examples/wanvideo/](./examples/wanvideo/)

https://github.com/user-attachments/assets/1d66ae74-3b02-40a9-acc3-ea95fc039314

<details>

<summary>å¿«é€Ÿå¼€å§‹</summary>

```python
import torch
from diffsynth import save_video
from diffsynth.pipelines.wan_video_new import WanVideoPipeline, ModelConfig

pipe = WanVideoPipeline.from_pretrained(
    torch_dtype=torch.bfloat16,
    device="cuda",
    model_configs=[
        ModelConfig(model_id="Wan-AI/Wan2.1-T2V-1.3B", origin_file_pattern="diffusion_pytorch_model*.safetensors", offload_device="cpu"),
        ModelConfig(model_id="Wan-AI/Wan2.1-T2V-1.3B", origin_file_pattern="models_t5_umt5-xxl-enc-bf16.pth", offload_device="cpu"),
        ModelConfig(model_id="Wan-AI/Wan2.1-T2V-1.3B", origin_file_pattern="Wan2.1_VAE.pth", offload_device="cpu"),
    ],
)
pipe.enable_vram_management()

video = pipe(
    prompt="çºªå®æ‘„å½±é£æ ¼ç”»é¢ï¼Œä¸€åªæ´»æ³¼çš„å°ç‹—åœ¨ç»¿èŒµèŒµçš„è‰åœ°ä¸Šè¿…é€Ÿå¥”è·‘ã€‚å°ç‹—æ¯›è‰²æ£•é»„ï¼Œä¸¤åªè€³æœµç«‹èµ·ï¼Œç¥æƒ…ä¸“æ³¨è€Œæ¬¢å¿«ã€‚é˜³å…‰æ´’åœ¨å®ƒèº«ä¸Šï¼Œä½¿å¾—æ¯›å‘çœ‹ä¸Šå»æ ¼å¤–æŸ”è½¯è€Œé—ªäº®ã€‚èƒŒæ™¯æ˜¯ä¸€ç‰‡å¼€é˜”çš„è‰åœ°ï¼Œå¶å°”ç‚¹ç¼€ç€å‡ æœµé‡èŠ±ï¼Œè¿œå¤„éšçº¦å¯è§è“å¤©å’Œå‡ ç‰‡ç™½äº‘ã€‚é€è§†æ„Ÿé²œæ˜ï¼Œæ•æ‰å°ç‹—å¥”è·‘æ—¶çš„åŠ¨æ„Ÿå’Œå››å‘¨è‰åœ°çš„ç”Ÿæœºã€‚ä¸­æ™¯ä¾§é¢ç§»åŠ¨è§†è§’ã€‚",
    negative_prompt="è‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£æ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œæ•´ä½“å‘ç°ï¼Œæœ€å·®è´¨é‡ï¼Œä½è´¨é‡ï¼ŒJPEGå‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œæ‰‹æŒ‡èåˆï¼Œé™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œæ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°",
    seed=0, tiled=True,
)
save_video(video, "video1.mp4", fps=15, quality=5)
```

</details>

<details>

<summary>æ¨¡å‹æ€»è§ˆ</summary>

|æ¨¡å‹ ID|é¢å¤–å‚æ•°|æ¨ç†|å…¨é‡è®­ç»ƒ|å…¨é‡è®­ç»ƒåéªŒè¯|LoRA è®­ç»ƒ|LoRA è®­ç»ƒåéªŒè¯|
|-|-|-|-|-|-|-|
|[Wan-AI/Wan2.1-T2V-1.3B](https://modelscope.cn/models/Wan-AI/Wan2.1-T2V-1.3B)||[code](./examples/wanvideo/model_inference/Wan2.1-T2V-1.3B.py)|[code](./examples/wanvideo/model_training/full/Wan2.1-T2V-1.3B.sh)|[code](./examples/wanvideo/model_training/validate_full/Wan2.1-T2V-1.3B.py)|[code](./examples/wanvideo/model_training/lora/Wan2.1-T2V-1.3B.sh)|[code](./examples/wanvideo/model_training/validate_lora/Wan2.1-T2V-1.3B.py)|
|[Wan-AI/Wan2.1-T2V-14B](https://modelscope.cn/models/Wan-AI/Wan2.1-T2V-14B)||[code](./examples/wanvideo/model_inference/Wan2.1-T2V-14B.py)|[code](./examples/wanvideo/model_training/full/Wan2.1-T2V-14B.sh)|[code](./examples/wanvideo/model_training/validate_full/Wan2.1-T2V-14B.py)|[code](./examples/wanvideo/model_training/lora/Wan2.1-T2V-14B.sh)|[code](./examples/wanvideo/model_training/validate_lora/Wan2.1-T2V-14B.py)|
|[Wan-AI/Wan2.1-I2V-14B-480P](https://modelscope.cn/models/Wan-AI/Wan2.1-I2V-14B-480P)|`input_image`|[code](./examples/wanvideo/model_inference/Wan2.1-I2V-14B-480P.py)|[code](./examples/wanvideo/model_training/full/Wan2.1-I2V-14B-480P.sh)|[code](./examples/wanvideo/model_training/validate_full/Wan2.1-I2V-14B-480P.py)|[code](./examples/wanvideo/model_training/lora/Wan2.1-I2V-14B-480P.sh)|[code](./examples/wanvideo/model_training/validate_lora/Wan2.1-I2V-14B-480P.py)|
|[Wan-AI/Wan2.1-I2V-14B-720P](https://modelscope.cn/models/Wan-AI/Wan2.1-I2V-14B-720P)|`input_image`|[code](./examples/wanvideo/model_inference/Wan2.1-I2V-14B-720P.py)|[code](./examples/wanvideo/model_training/full/Wan2.1-I2V-14B-720P.sh)|[code](./examples/wanvideo/model_training/validate_full/Wan2.1-I2V-14B-720P.py)|[code](./examples/wanvideo/model_training/lora/Wan2.1-I2V-14B-720P.sh)|[code](./examples/wanvideo/model_training/validate_lora/Wan2.1-I2V-14B-720P.py)|
|[Wan-AI/Wan2.1-FLF2V-14B-720P](https://modelscope.cn/models/Wan-AI/Wan2.1-FLF2V-14B-720P)|`input_image`, `end_image`|[code](./examples/wanvideo/model_inference/Wan2.1-FLF2V-14B-720P.py)|[code](./examples/wanvideo/model_training/full/Wan2.1-FLF2V-14B-720P.sh)|[code](./examples/wanvideo/model_training/validate_full/Wan2.1-FLF2V-14B-720P.py)|[code](./examples/wanvideo/model_training/lora/Wan2.1-FLF2V-14B-720P.sh)|[code](./examples/wanvideo/model_training/validate_lora/Wan2.1-FLF2V-14B-720P.py)|
|[PAI/Wan2.1-Fun-1.3B-InP](https://modelscope.cn/models/PAI/Wan2.1-Fun-1.3B-InP)|`input_image`, `end_image`|[code](./examples/wanvideo/model_inference/Wan2.1-Fun-1.3B-InP.py)|[code](./examples/wanvideo/model_training/full/Wan2.1-Fun-1.3B-InP.sh)|[code](./examples/wanvideo/model_training/validate_full/Wan2.1-Fun-1.3B-InP.py)|[code](./examples/wanvideo/model_training/lora/Wan2.1-Fun-1.3B-InP.sh)|[code](./examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-1.3B-InP.py)|
|[PAI/Wan2.1-Fun-1.3B-Control](https://modelscope.cn/models/PAI/Wan2.1-Fun-1.3B-Control)|`control_video`|[code](./examples/wanvideo/model_inference/Wan2.1-Fun-1.3B-Control.py)|[code](./examples/wanvideo/model_training/full/Wan2.1-Fun-1.3B-Control.sh)|[code](./examples/wanvideo/model_training/validate_full/Wan2.1-Fun-1.3B-Control.py)|[code](./examples/wanvideo/model_training/lora/Wan2.1-Fun-1.3B-Control.sh)|[code](./examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-1.3B-Control.py)|
|[PAI/Wan2.1-Fun-14B-InP](https://modelscope.cn/models/PAI/Wan2.1-Fun-14B-InP)|`input_image`, `end_image`|[code](./examples/wanvideo/model_inference/Wan2.1-Fun-14B-InP.py)|[code](./examples/wanvideo/model_training/full/Wan2.1-Fun-14B-InP.sh)|[code](./examples/wanvideo/model_training/validate_full/Wan2.1-Fun-14B-InP.py)|[code](./examples/wanvideo/model_training/lora/Wan2.1-Fun-14B-InP.sh)|[code](./examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-14B-InP.py)|
|[PAI/Wan2.1-Fun-14B-Control](https://modelscope.cn/models/PAI/Wan2.1-Fun-14B-Control)|`control_video`|[code](./examples/wanvideo/model_inference/Wan2.1-Fun-14B-Control.py)|[code](./examples/wanvideo/model_training/full/Wan2.1-Fun-14B-Control.sh)|[code](./examples/wanvideo/model_training/validate_full/Wan2.1-Fun-14B-Control.py)|[code](./examples/wanvideo/model_training/lora/Wan2.1-Fun-14B-Control.sh)|[code](./examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-14B-Control.py)|
|[PAI/Wan2.1-Fun-V1.1-1.3B-Control](https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-1.3B-Control)|`control_video`, `reference_image`|[code](./examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-1.3B-Control.py)|[code](./examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-1.3B-Control.sh)|[code](./examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-1.3B-Control.py)|[code](./examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-1.3B-Control.sh)|[code](./examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-V1.1-1.3B-Control.py)|
|[PAI/Wan2.1-Fun-V1.1-14B-Control](https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-14B-Control)|`control_video`, `reference_image`|[code](./examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-14B-Control.py)|[code](./examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-14B-Control.sh)|[code](./examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-14B-Control.py)|[code](./examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-14B-Control.sh)|[code](./examples/wanvideo/examples/wanmodel_training/validate_lora/Wan2.1-Fun-V1.1-14B-Control.py)|
|[PAI/Wan2.1-Fun-V1.1-1.3B-InP](https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-1.3B-InP)|`input_image`, `end_image`|[code](./examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-1.3B-InP.py)|[code](./examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-1.3B-InP.sh)|[code](./examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-1.3B-InP.py)|[code](./examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-1.3B-InP.sh)|[code](./examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-V1.1-1.3B-InP.py)|
|[PAI/Wan2.1-Fun-V1.1-14B-InP](https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-14B-InP)|`input_image`, `end_image`|[code](./examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-14B-InP.py)|[code](./examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-14B-InP.sh)|[code](./examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-14B-InP.py)|[code](./examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-14B-InP.sh)|[code](./examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-V1.1-14B-InP.py)|
|[PAI/Wan2.1-Fun-V1.1-1.3B-Control-Camera](https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-1.3B-Control-Camera)|`control_camera_video`, `input_image`|[code](./examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-1.3B-Control-Camera.py)|[code](./examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-1.3B-Control-Camera.sh)|[code](./examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-1.3B-Control-Camera.py)|[code](./examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-1.3B-Control-Camera.sh)|[code](./examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-V1.1-1.3B-Control-Camera.py)|
|[PAI/Wan2.1-Fun-V1.1-14B-Control-Camera](https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-14B-Control-Camera)|`control_camera_video`, `input_image`|[code](./examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-14B-Control-Camera.py)|[code](./examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-14B-Control-Camera.sh)|[code](./examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-14B-Control-Camera.py)|[code](./examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-14B-Control-Camera.sh)|[code](./examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-V1.1-14B-Control-Camera.py)|
|[iic/VACE-Wan2.1-1.3B-Preview](https://modelscope.cn/models/iic/VACE-Wan2.1-1.3B-Preview)|`vace_control_video`, `vace_reference_image`|[code](./examples/wanvideo/model_inference/Wan2.1-VACE-1.3B-Preview.py)|[code](./examples/wanvideo/model_training/full/Wan2.1-VACE-1.3B-Preview.sh)|[code](./examples/wanvideo/model_training/validate_full/Wan2.1-VACE-1.3B-Preview.py)|[code](./examples/wanvideo/model_training/lora/Wan2.1-VACE-1.3B-Preview.sh)|[code](./examples/wanvideo/model_training/validate_lora/Wan2.1-VACE-1.3B-Preview.py)|
|[Wan-AI/Wan2.1-VACE-1.3B](https://modelscope.cn/models/Wan-AI/Wan2.1-VACE-1.3B)|`vace_control_video`, `vace_reference_image`|[code](./examples/wanvideo/model_inference/Wan2.1-VACE-1.3B.py)|[code](./examples/wanvideo/model_training/full/Wan2.1-VACE-1.3B.sh)|[code](./examples/wanvideo/model_training/validate_full/Wan2.1-VACE-1.3B.py)|[code](./examples/wanvideo/model_training/lora/Wan2.1-VACE-1.3B.sh)|[code](./examples/wanvideo/model_training/validate_lora/Wan2.1-VACE-1.3B.py)|
|[Wan-AI/Wan2.1-VACE-14B](https://modelscope.cn/models/Wan-AI/Wan2.1-VACE-14B)|`vace_control_video`, `vace_reference_image`|[code](./examples/wanvideo/model_inference/Wan2.1-VACE-14B.py)|[code](./examples/wanvideo/model_training/full/Wan2.1-VACE-14B.sh)|[code](./examples/wanvideo/model_training/validate_full/Wan2.1-VACE-14B.py)|[code](./examples/wanvideo/model_training/lora/Wan2.1-VACE-14B.sh)|[code](./examples/wanvideo/model_training/validate_lora/Wan2.1-VACE-14B.py)|
|[DiffSynth-Studio/Wan2.1-1.3b-speedcontrol-v1](https://modelscope.cn/models/DiffSynth-Studio/Wan2.1-1.3b-speedcontrol-v1)|`motion_bucket_id`|[code](./examples/wanvideo/model_inference/Wan2.1-1.3b-speedcontrol-v1.py)|[code](./examples/wanvideo/model_training/full/Wan2.1-1.3b-speedcontrol-v1.sh)|[code](./examples/wanvideo/model_training/validate_full/Wan2.1-1.3b-speedcontrol-v1.py)|[code](./examples/wanvideo/model_training/lora/Wan2.1-1.3b-speedcontrol-v1.sh)|[code](./examples/wanvideo/model_training/validate_lora/Wan2.1-1.3b-speedcontrol-v1.py)|

</details>



### æ›´å¤šæ¨¡å‹



<details>
<summary>å›¾åƒç”Ÿæˆæ¨¡å‹</summary>

è¯¦ç»†é¡µé¢ï¼š[./examples/image_synthesis/](./examples/image_synthesis/)

|FLUX|Stable Diffusion 3|
|-|-|
|![image_1024_cfg](https://github.com/user-attachments/assets/984561e9-553d-4952-9443-79ce144f379f)|![image_1024](https://github.com/modelscope/DiffSynth-Studio/assets/35051019/4df346db-6f91-420a-b4c1-26e205376098)|

|Kolors|Hunyuan-DiT|
|-|-|
|![image_1024](https://github.com/modelscope/DiffSynth-Studio/assets/35051019/53ef6f41-da11-4701-8665-9f64392607bf)|![image_1024](https://github.com/modelscope/DiffSynth-Studio/assets/35051019/60b022c8-df3f-4541-95ab-bf39f2fa8bb5)|

|Stable Diffusion|Stable Diffusion XL|
|-|-|
|![1024](https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/6fc84611-8da6-4a1f-8fee-9a34eba3b4a5)|![1024](https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/67687748-e738-438c-aee5-96096f09ac90)|

</details>



<details>
<summary>è§†é¢‘ç”Ÿæˆæ¨¡å‹</summary>

- HunyuanVideoï¼š[./examples/HunyuanVideo/](./examples/HunyuanVideo/)

https://github.com/user-attachments/assets/48dd24bb-0cc6-40d2-88c3-10feed3267e9

- StepVideoï¼š[./examples/stepvideo/](./examples/stepvideo/)

https://github.com/user-attachments/assets/5954fdaa-a3cf-45a3-bd35-886e3cc4581b

- CogVideoXï¼š[./examples/CogVideoX/](./examples/CogVideoX/)

https://github.com/user-attachments/assets/26b044c1-4a60-44a4-842f-627ff289d006

</details>



<details>
<summary>å›¾åƒè´¨é‡è¯„ä¼°æ¨¡å‹</summary>

æˆ‘ä»¬é›†æˆäº†ä¸€ç³»åˆ—å›¾åƒè´¨é‡è¯„ä¼°æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥ç”¨äºå›¾åƒç”Ÿæˆæ¨¡å‹çš„è¯„æµ‹ã€å¯¹é½è®­ç»ƒç­‰åœºæ™¯ä¸­ã€‚

è¯¦ç»†é¡µé¢ï¼š[./examples/image_quality_metric/](./examples/image_quality_metric/)

* [ImageReward](https://github.com/THUDM/ImageReward)
* [Aesthetic](https://github.com/christophschuhmann/improved-aesthetic-predictor)
* [PickScore](https://github.com/yuvalkirstain/pickscore)
* [CLIP](https://github.com/openai/CLIP)
* [HPSv2](https://github.com/tgxs002/HPSv2)
* [HPSv2.1](https://github.com/tgxs002/HPSv2)
* [MPS](https://github.com/Kwai-Kolors/MPS)

</details>



## åˆ›æ–°æˆæœ

DiffSynth-Studio ä¸ä»…ä»…æ˜¯ä¸€ä¸ªå·¥ç¨‹åŒ–çš„æ¨¡å‹æ¡†æ¶ï¼Œæ›´æ˜¯åˆ›æ–°æˆæœçš„å­µåŒ–å™¨ã€‚

<details>
<summary>Nexus-Gen: ç»Ÿä¸€æ¶æ„çš„å›¾åƒç†è§£ã€ç”Ÿæˆã€ç¼–è¾‘</summary>

- è¯¦ç»†é¡µé¢ï¼šhttps://github.com/modelscope/Nexus-Gen
- è®ºæ–‡ï¼š[Nexus-Gen: Unified Image Understanding, Generation, and Editing via Prefilled Autoregression in Shared Embedding Space](https://arxiv.org/pdf/2504.21356)
- æ¨¡å‹ï¼š[ModelScope](https://www.modelscope.cn/models/DiffSynth-Studio/Nexus-GenV2), [HuggingFace](https://huggingface.co/modelscope/Nexus-GenV2)
- æ•°æ®é›†ï¼š[ModelScope Dataset](https://www.modelscope.cn/datasets/DiffSynth-Studio/Nexus-Gen-Training-Dataset)
- åœ¨çº¿ä½“éªŒï¼š[ModelScope Nexus-Gen Studio](https://www.modelscope.cn/studios/DiffSynth-Studio/Nexus-Gen)

![](https://github.com/modelscope/Nexus-Gen/raw/main/assets/illustrations/gen_edit.jpg)

</details>



<details>
<summary>ArtAug: å›¾åƒç”Ÿæˆæ¨¡å‹çš„ç¾å­¦æå‡</summary>

- è¯¦ç»†é¡µé¢ï¼š[./examples/ArtAug/](./examples/ArtAug/)
- è®ºæ–‡ï¼š[ArtAug: Enhancing Text-to-Image Generation through Synthesis-Understanding Interaction](https://arxiv.org/abs/2412.12888)
- æ¨¡å‹ï¼š[ModelScope](https://www.modelscope.cn/models/DiffSynth-Studio/ArtAug-lora-FLUX.1dev-v1), [HuggingFace](https://huggingface.co/ECNU-CILab/ArtAug-lora-FLUX.1dev-v1)
- åœ¨çº¿ä½“éªŒï¼š[ModelScope AIGC Tab](https://www.modelscope.cn/aigc/imageGeneration?tab=advanced&versionId=7228&modelType=LoRA&sdVersion=FLUX_1&modelUrl=modelscope%3A%2F%2FDiffSynth-Studio%2FArtAug-lora-FLUX.1dev-v1%3Frevision%3Dv1.0)

|FLUX.1-dev|FLUX.1-dev + ArtAug LoRA|
|-|-|
|![image_1_base](https://github.com/user-attachments/assets/e1d5c505-b423-45fe-be01-25c2758f5417)|![image_1_enhance](https://github.com/user-attachments/assets/335908e3-d0bd-41c2-9d99-d10528a2d719)|

</details>



<details>

<summary>EliGen: ç²¾å‡†çš„å›¾åƒåˆ†åŒºæ§åˆ¶</summary>

- è¯¦ç»†é¡µé¢ï¼š[./examples/EntityControl/](./examples/EntityControl/)
- è®ºæ–‡ï¼š[EliGen: Entity-Level Controlled Image Generation with Regional Attention](https://arxiv.org/abs/2501.01097)
- æ¨¡å‹ï¼š[ModelScope](https://www.modelscope.cn/models/DiffSynth-Studio/Eligen), [HuggingFace](https://huggingface.co/modelscope/EliGen)
- åœ¨çº¿ä½“éªŒï¼š[ModelScope EliGen Studio](https://www.modelscope.cn/studios/DiffSynth-Studio/EliGen)
- æ•°æ®é›†ï¼š[EliGen Train Set](https://www.modelscope.cn/datasets/DiffSynth-Studio/EliGenTrainSet)

|å®ä½“æ§åˆ¶åŒºåŸŸ|ç”Ÿæˆå›¾åƒ|
|-|-|
|![eligen_example_2_mask_0](https://github.com/user-attachments/assets/1c6d9445-5022-4d91-ad2e-dc05321883d1)|![eligen_example_2_0](https://github.com/user-attachments/assets/86739945-cb07-4a49-b3b3-3bb65c90d14f)|

</details>



<details>

<summary>ExVideo: è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„æ‰©å±•è®­ç»ƒ</summary>

- é¡¹ç›®é¡µé¢ï¼š[Project Page](https://ecnu-cilab.github.io/ExVideoProjectPage/)
- è®ºæ–‡ï¼š[ExVideo: Extending Video Diffusion Models via Parameter-Efficient Post-Tuning](https://arxiv.org/abs/2406.14130)
- ä»£ç æ ·ä¾‹ï¼š[./examples/ExVideo/](./examples/ExVideo/)
- æ¨¡å‹ï¼š[ModelScope](https://modelscope.cn/models/ECNU-CILab/ExVideo-SVD-128f-v1), [HuggingFace](https://huggingface.co/ECNU-CILab/ExVideo-SVD-128f-v1)

https://github.com/modelscope/DiffSynth-Studio/assets/35051019/d97f6aa9-8064-4b5b-9d49-ed6001bb9acc

</details>



<details>

<summary>Diffutoon: é«˜åˆ†è¾¨ç‡åŠ¨æ¼«é£æ ¼è§†é¢‘æ¸²æŸ“</summary>

- é¡¹ç›®é¡µé¢ï¼š[Project Page](https://ecnu-cilab.github.io/DiffutoonProjectPage/)
- è®ºæ–‡ï¼š[Diffutoon: High-Resolution Editable Toon Shading via Diffusion Models](https://arxiv.org/abs/2401.16224)
- ä»£ç æ ·ä¾‹ï¼š[./examples/Diffutoon/](./examples/Diffutoon/)

https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/b54c05c5-d747-4709-be5e-b39af82404dd

</details>



<details>

<summary>DiffSynth: æœ¬é¡¹ç›®çš„åˆä»£ç‰ˆæœ¬</summary>

- é¡¹ç›®é¡µé¢ï¼š[Project Page](https://ecnu-cilab.github.io/DiffSynth.github.io/)
- è®ºæ–‡ï¼š[DiffSynth: Latent In-Iteration Deflickering for Realistic Video Synthesis](https://arxiv.org/abs/2308.03463)
- ä»£ç æ ·ä¾‹ï¼š[./examples/diffsynth/](./examples/diffsynth/)

https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/59fb2f7b-8de0-4481-b79f-0c3a7361a1ea

</details>



## æ›´æ–°å†å²

- **2025å¹´7æœˆ11æ—¥** ğŸ”¥ğŸ”¥ğŸ”¥ æˆ‘ä»¬æå‡º Nexus-Genï¼Œä¸€ä¸ªå°†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­è¨€æ¨ç†èƒ½åŠ›ä¸æ‰©æ•£æ¨¡å‹çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ç›¸ç»“åˆçš„ç»Ÿä¸€æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ”¯æŒæ— ç¼çš„å›¾åƒç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ã€‚
  - è®ºæ–‡: [Nexus-Gen: Unified Image Understanding, Generation, and Editing via Prefilled Autoregression in Shared Embedding Space](https://arxiv.org/pdf/2504.21356)
  - Github ä»“åº“: https://github.com/modelscope/Nexus-Gen
  - æ¨¡å‹: [ModelScope](https://www.modelscope.cn/models/DiffSynth-Studio/Nexus-GenV2), [HuggingFace](https://huggingface.co/modelscope/Nexus-GenV2)
  - è®­ç»ƒæ•°æ®é›†: [ModelScope Dataset](https://www.modelscope.cn/datasets/DiffSynth-Studio/Nexus-Gen-Training-Dataset)
  - åœ¨çº¿ä½“éªŒ: [ModelScope Nexus-Gen Studio](https://www.modelscope.cn/studios/DiffSynth-Studio/Nexus-Gen)

- **2025å¹´6æœˆ15æ—¥** ModelScope å®˜æ–¹è¯„æµ‹æ¡†æ¶ [EvalScope](https://github.com/modelscope/evalscope) ç°å·²æ”¯æŒæ–‡ç”Ÿå›¾ç”Ÿæˆè¯„æµ‹ã€‚è¯·å‚è€ƒ[æœ€ä½³å®è·µ](https://evalscope.readthedocs.io/zh-cn/latest/best_practice/t2i_eval.html)æŒ‡å—è¿›è¡Œå°è¯•ã€‚

- **2025å¹´3æœˆ25æ—¥** æˆ‘ä»¬çš„æ–°å¼€æºé¡¹ç›® [DiffSynth-Engine](https://github.com/modelscope/DiffSynth-Engine) ç°å·²å¼€æºï¼ä¸“æ³¨äºç¨³å®šçš„æ¨¡å‹éƒ¨ç½²ï¼Œé¢å‘å·¥ä¸šç•Œï¼Œæä¾›æ›´å¥½çš„å·¥ç¨‹æ”¯æŒã€æ›´é«˜çš„è®¡ç®—æ€§èƒ½å’Œæ›´ç¨³å®šçš„åŠŸèƒ½ã€‚

<details>
<summary>æ›´å¤š</summary>

- **2025å¹´3æœˆ31æ—¥** æˆ‘ä»¬æ”¯æŒ InfiniteYouï¼Œä¸€ç§ç”¨äº FLUX çš„äººè„¸ç‰¹å¾ä¿ç•™æ–¹æ³•ã€‚æ›´å¤šç»†èŠ‚è¯·å‚è€ƒ [./examples/InfiniteYou/](./examples/InfiniteYou/)ã€‚

- **2025å¹´3æœˆ13æ—¥** æˆ‘ä»¬æ”¯æŒ HunyuanVideo-I2Vï¼Œå³è…¾è®¯å¼€æºçš„ HunyuanVideo çš„å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆç‰ˆæœ¬ã€‚æ›´å¤šç»†èŠ‚è¯·å‚è€ƒ [./examples/HunyuanVideo/](./examples/HunyuanVideo/)ã€‚

- **2025å¹´2æœˆ25æ—¥** æˆ‘ä»¬æ”¯æŒ Wan-Videoï¼Œè¿™æ˜¯é˜¿é‡Œå·´å·´å¼€æºçš„ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„è§†é¢‘åˆæˆæ¨¡å‹ã€‚è¯¦è§ [./examples/wanvideo/](./examples/wanvideo/)ã€‚

- **2025å¹´2æœˆ17æ—¥** æˆ‘ä»¬æ”¯æŒ [StepVideo](https://modelscope.cn/models/stepfun-ai/stepvideo-t2v/summary)ï¼å…ˆè¿›çš„è§†é¢‘åˆæˆæ¨¡å‹ï¼è¯¦è§ [./examples/stepvideo](./examples/stepvideo/)ã€‚

- **2024å¹´12æœˆ31æ—¥** æˆ‘ä»¬æå‡º EliGenï¼Œä¸€ç§ç”¨äºç²¾ç¡®å®ä½“çº§åˆ«æ§åˆ¶çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æ–°æ¡†æ¶ï¼Œå¹¶è¾…ä»¥ä¿®å¤èåˆç®¡é“ï¼Œå°†å…¶èƒ½åŠ›æ‰©å±•åˆ°å›¾åƒä¿®å¤ä»»åŠ¡ã€‚EliGen å¯ä»¥æ— ç¼é›†æˆç°æœ‰çš„ç¤¾åŒºæ¨¡å‹ï¼Œå¦‚ IP-Adapter å’Œ In-Context LoRAï¼Œæå‡å…¶é€šç”¨æ€§ã€‚æ›´å¤šè¯¦æƒ…ï¼Œè¯·è§ [./examples/EntityControl](./examples/EntityControl/)ã€‚
  - è®ºæ–‡: [EliGen: Entity-Level Controlled Image Generation with Regional Attention](https://arxiv.org/abs/2501.01097)
  - æ¨¡å‹: [ModelScope](https://www.modelscope.cn/models/DiffSynth-Studio/Eligen), [HuggingFace](https://huggingface.co/modelscope/EliGen)
  - åœ¨çº¿ä½“éªŒ: [ModelScope EliGen Studio](https://www.modelscope.cn/studios/DiffSynth-Studio/EliGen)
  - è®­ç»ƒæ•°æ®é›†: [EliGen Train Set](https://www.modelscope.cn/datasets/DiffSynth-Studio/EliGenTrainSet)

- **2024å¹´12æœˆ19æ—¥** æˆ‘ä»¬ä¸º HunyuanVideo å®ç°äº†é«˜çº§æ˜¾å­˜ç®¡ç†ï¼Œä½¿å¾—åœ¨ 24GB æ˜¾å­˜ä¸‹å¯ä»¥ç”Ÿæˆåˆ†è¾¨ç‡ä¸º 129x720x1280 çš„è§†é¢‘ï¼Œæˆ–åœ¨ä»… 6GB æ˜¾å­˜ä¸‹ç”Ÿæˆåˆ†è¾¨ç‡ä¸º 129x512x384 çš„è§†é¢‘ã€‚æ›´å¤šç»†èŠ‚è¯·å‚è€ƒ [./examples/HunyuanVideo/](./examples/HunyuanVideo/)ã€‚

- **2024å¹´12æœˆ18æ—¥** æˆ‘ä»¬æå‡º ArtAugï¼Œä¸€ç§é€šè¿‡åˆæˆ-ç†è§£äº¤äº’æ¥æ”¹è¿›æ–‡ç”Ÿå›¾æ¨¡å‹çš„æ–¹æ³•ã€‚æˆ‘ä»¬ä»¥ LoRA æ ¼å¼ä¸º FLUX.1-dev è®­ç»ƒäº†ä¸€ä¸ª ArtAug å¢å¼ºæ¨¡å—ã€‚è¯¥æ¨¡å‹å°† Qwen2-VL-72B çš„ç¾å­¦ç†è§£èå…¥ FLUX.1-devï¼Œä»è€Œæå‡äº†ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚
  - è®ºæ–‡: https://arxiv.org/abs/2412.12888
  - ç¤ºä¾‹: https://github.com/modelscope/DiffSynth-Studio/tree/main/examples/ArtAug
  - æ¨¡å‹: [ModelScope](https://www.modelscope.cn/models/DiffSynth-Studio/ArtAug-lora-FLUX.1dev-v1), [HuggingFace](https://huggingface.co/ECNU-CILab/ArtAug-lora-FLUX.1dev-v1)
  - æ¼”ç¤º: [ModelScope](https://modelscope.cn/aigc/imageGeneration?tab=advanced&versionId=7228&modelType=LoRA&sdVersion=FLUX_1&modelUrl=modelscope%3A%2F%2FDiffSynth-Studio%2FArtAug-lora-FLUX.1dev-v1%3Frevision%3Dv1.0), HuggingFace (å³å°†ä¸Šçº¿)

- **2024å¹´10æœˆ25æ—¥** æˆ‘ä»¬æä¾›äº†å¹¿æ³›çš„ FLUX ControlNet æ”¯æŒã€‚è¯¥é¡¹ç›®æ”¯æŒè®¸å¤šä¸åŒçš„ ControlNet æ¨¡å‹ï¼Œå¹¶ä¸”å¯ä»¥è‡ªç”±ç»„åˆï¼Œå³ä½¿å®ƒä»¬çš„ç»“æ„ä¸åŒã€‚æ­¤å¤–ï¼ŒControlNet æ¨¡å‹å…¼å®¹é«˜åˆ†è¾¨ç‡ä¼˜åŒ–å’Œåˆ†åŒºæ§åˆ¶æŠ€æœ¯ï¼Œèƒ½å¤Ÿå®ç°éå¸¸å¼ºå¤§çš„å¯æ§å›¾åƒç”Ÿæˆã€‚è¯¦è§ [`./examples/ControlNet/`](./examples/ControlNet/)ã€‚

- **2024å¹´10æœˆ8æ—¥** æˆ‘ä»¬å‘å¸ƒäº†åŸºäº CogVideoX-5B å’Œ ExVideo çš„æ‰©å±• LoRAã€‚æ‚¨å¯ä»¥ä» [ModelScope](https://modelscope.cn/models/ECNU-CILab/ExVideo-CogVideoX-LoRA-129f-v1) æˆ– [HuggingFace](https://huggingface.co/ECNU-CILab/ExVideo-CogVideoX-LoRA-129f-v1) ä¸‹è½½æ­¤æ¨¡å‹ã€‚

- **2024å¹´8æœˆ22æ—¥** æœ¬é¡¹ç›®ç°å·²æ”¯æŒ CogVideoX-5Bã€‚è¯¦è§ [æ­¤å¤„](/examples/video_synthesis/)ã€‚æˆ‘ä»¬ä¸ºè¿™ä¸ªæ–‡ç”Ÿè§†é¢‘æ¨¡å‹æä¾›äº†å‡ ä¸ªæœ‰è¶£çš„åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
  - æ–‡æœ¬åˆ°è§†é¢‘
  - è§†é¢‘ç¼–è¾‘
  - è‡ªæˆ‘è¶…åˆ†
  - è§†é¢‘æ’å¸§

- **2024å¹´8æœˆ22æ—¥** æˆ‘ä»¬å®ç°äº†ä¸€ä¸ªæœ‰è¶£çš„ç”»ç¬”åŠŸèƒ½ï¼Œæ”¯æŒæ‰€æœ‰æ–‡ç”Ÿå›¾æ¨¡å‹ã€‚ç°åœ¨ï¼Œæ‚¨å¯ä»¥åœ¨ AI çš„è¾…åŠ©ä¸‹ä½¿ç”¨ç”»ç¬”åˆ›ä½œæƒŠè‰³çš„å›¾åƒäº†ï¼
  - åœ¨æˆ‘ä»¬çš„ [WebUI](#usage-in-webui) ä¸­ä½¿ç”¨å®ƒã€‚

- **2024å¹´8æœˆ21æ—¥** DiffSynth-Studio ç°å·²æ”¯æŒ FLUXã€‚
  - å¯ç”¨ CFG å’Œé«˜åˆ†è¾¨ç‡ä¿®å¤ä»¥æå‡è§†è§‰è´¨é‡ã€‚è¯¦è§ [æ­¤å¤„](/examples/image_synthesis/README.md)
  - LoRAã€ControlNet å’Œå…¶ä»–é™„åŠ æ¨¡å‹å°†å¾ˆå¿«æ¨å‡ºã€‚

- **2024å¹´6æœˆ21æ—¥** æˆ‘ä»¬æå‡º ExVideoï¼Œä¸€ç§æ—¨åœ¨å¢å¼ºè§†é¢‘ç”Ÿæˆæ¨¡å‹èƒ½åŠ›çš„åè®­ç»ƒå¾®è°ƒæŠ€æœ¯ã€‚æˆ‘ä»¬å°† Stable Video Diffusion è¿›è¡Œäº†æ‰©å±•ï¼Œå®ç°äº†é•¿è¾¾ 128 å¸§çš„é•¿è§†é¢‘ç”Ÿæˆã€‚
  - [é¡¹ç›®é¡µé¢](https://ecnu-cilab.github.io/ExVideoProjectPage/)
  - æºä»£ç å·²åœ¨æ­¤ä»“åº“ä¸­å‘å¸ƒã€‚è¯¦è§ [`examples/ExVideo`](./examples/ExVideo/)ã€‚
  - æ¨¡å‹å·²å‘å¸ƒäº [HuggingFace](https://huggingface.co/ECNU-CILab/ExVideo-SVD-128f-v1) å’Œ [ModelScope](https://modelscope.cn/models/ECNU-CILab/ExVideo-SVD-128f-v1)ã€‚
  - æŠ€æœ¯æŠ¥å‘Šå·²å‘å¸ƒäº [arXiv](https://arxiv.org/abs/2406.14130)ã€‚
  - æ‚¨å¯ä»¥åœ¨æ­¤ [æ¼”ç¤º](https://huggingface.co/spaces/modelscope/ExVideo-SVD-128f-v1) ä¸­è¯•ç”¨ ExVideoï¼

- **2024å¹´6æœˆ13æ—¥** DiffSynth Studio å·²è¿ç§»è‡³ ModelScopeã€‚å¼€å‘å›¢é˜Ÿä¹Ÿä»â€œæˆ‘â€è½¬å˜ä¸ºâ€œæˆ‘ä»¬â€ã€‚å½“ç„¶ï¼Œæˆ‘ä»ä¼šå‚ä¸åç»­çš„å¼€å‘å’Œç»´æŠ¤å·¥ä½œã€‚

- **2024å¹´1æœˆ29æ—¥** æˆ‘ä»¬æå‡º Diffutoonï¼Œè¿™æ˜¯ä¸€ä¸ªå‡ºè‰²çš„å¡é€šç€è‰²è§£å†³æ–¹æ¡ˆã€‚
  - [é¡¹ç›®é¡µé¢](https://ecnu-cilab.github.io/DiffutoonProjectPage/)
  - æºä»£ç å·²åœ¨æ­¤é¡¹ç›®ä¸­å‘å¸ƒã€‚
  - æŠ€æœ¯æŠ¥å‘Šï¼ˆIJCAI 2024ï¼‰å·²å‘å¸ƒäº [arXiv](https://arxiv.org/abs/2401.16224)ã€‚

- **2023å¹´12æœˆ8æ—¥** æˆ‘ä»¬å†³å®šå¯åŠ¨ä¸€ä¸ªæ–°é¡¹ç›®ï¼Œæ—¨åœ¨é‡Šæ”¾æ‰©æ•£æ¨¡å‹çš„æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨è§†é¢‘åˆæˆæ–¹é¢ã€‚è¯¥é¡¹ç›®çš„å¼€å‘å·¥ä½œæ­£å¼å¼€å§‹ã€‚

- **2023å¹´11æœˆ15æ—¥** æˆ‘ä»¬æå‡º FastBlendï¼Œä¸€ç§å¼ºå¤§çš„è§†é¢‘å»é—ªçƒç®—æ³•ã€‚
  - sd-webui æ‰©å±•å·²å‘å¸ƒäº [GitHub](https://github.com/Artiprocher/sd-webui-fastblend)ã€‚
  - æ¼”ç¤ºè§†é¢‘å·²åœ¨ Bilibili ä¸Šå±•ç¤ºï¼ŒåŒ…å«ä¸‰ä¸ªä»»åŠ¡ï¼š
    - [è§†é¢‘å»é—ªçƒ](https://www.bilibili.com/video/BV1d94y1W7PE)
    - [è§†é¢‘æ’å¸§](https://www.bilibili.com/video/BV1Lw411m71p)
    - [å›¾åƒé©±åŠ¨çš„è§†é¢‘æ¸²æŸ“](https://www.bilibili.com/video/BV1RB4y1Z7LF)
  - æŠ€æœ¯æŠ¥å‘Šå·²å‘å¸ƒäº [arXiv](https://arxiv.org/abs/2311.09265)ã€‚
  - å…¶ä»–ç”¨æˆ·å¼€å‘çš„éå®˜æ–¹ ComfyUI æ‰©å±•å·²å‘å¸ƒäº [GitHub](https://github.com/AInseven/ComfyUI-fastblend)ã€‚

- **2023å¹´10æœˆ1æ—¥** æˆ‘ä»¬å‘å¸ƒäº†è¯¥é¡¹ç›®çš„æ—©æœŸç‰ˆæœ¬ï¼Œåä¸º FastSDXLã€‚è¿™æ˜¯æ„å»ºä¸€ä¸ªæ‰©æ•£å¼•æ“çš„åˆæ­¥å°è¯•ã€‚
  - æºä»£ç å·²å‘å¸ƒäº [GitHub](https://github.com/Artiprocher/FastSDXL)ã€‚
  - FastSDXL åŒ…å«ä¸€ä¸ªå¯è®­ç»ƒçš„ OLSS è°ƒåº¦å™¨ï¼Œä»¥æé«˜æ•ˆç‡ã€‚
    - OLSS çš„åŸå§‹ä»“åº“ä½äº [æ­¤å¤„](https://github.com/alibaba/EasyNLP/tree/master/diffusion/olss_scheduler)ã€‚
    - æŠ€æœ¯æŠ¥å‘Šï¼ˆCIKM 2023ï¼‰å·²å‘å¸ƒäº [arXiv](https://arxiv.org/abs/2305.14677)ã€‚
    - æ¼”ç¤ºè§†é¢‘å·²å‘å¸ƒäº [Bilibili](https://www.bilibili.com/video/BV1w8411y7uj)ã€‚
    - ç”±äº OLSS éœ€è¦é¢å¤–è®­ç»ƒï¼Œæˆ‘ä»¬æœªåœ¨æœ¬é¡¹ç›®ä¸­å®ç°å®ƒã€‚

- **2023å¹´8æœˆ29æ—¥** æˆ‘ä»¬æå‡º DiffSynthï¼Œä¸€ä¸ªè§†é¢‘åˆæˆæ¡†æ¶ã€‚
  - [é¡¹ç›®é¡µé¢](https://ecnu-cilab.github.io/DiffSynth.github.io/)ã€‚
  - æºä»£ç å·²å‘å¸ƒåœ¨ [EasyNLP](https://github.com/alibaba/EasyNLP/tree/master/diffusion/DiffSynth)ã€‚
  - æŠ€æœ¯æŠ¥å‘Šï¼ˆECML PKDD 2024ï¼‰å·²å‘å¸ƒäº [arXiv](https://arxiv.org/abs/2308.03463)ã€‚

</details>
