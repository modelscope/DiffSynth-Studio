# DiffSynth-Studio

<a href="https://github.com/modelscope/DiffSynth-Studio"><img src=".github/workflows/logo.gif" title="Logo" style="max-width:100%;" width="55" /></a> <a href="https://trendshift.io/repositories/10946" target="_blank"><img src="https://trendshift.io/api/badge/repositories/10946" alt="modelscope%2FDiffSynth-Studio | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a></p>

[![PyPI](https://img.shields.io/pypi/v/DiffSynth)](https://pypi.org/project/DiffSynth/)
[![license](https://img.shields.io/github/license/modelscope/DiffSynth-Studio.svg)](https://github.com/modelscope/DiffSynth-Studio/blob/master/LICENSE)
[![open issues](https://isitmaintained.com/badge/open/modelscope/DiffSynth-Studio.svg)](https://github.com/modelscope/DiffSynth-Studio/issues)
[![GitHub pull-requests](https://img.shields.io/github/issues-pr/modelscope/DiffSynth-Studio.svg)](https://GitHub.com/modelscope/DiffSynth-Studio/pull/)
[![GitHub latest commit](https://badgen.net/github/last-commit/modelscope/DiffSynth-Studio)](https://GitHub.com/modelscope/DiffSynth-Studio/commit/)

[Switch to English](./README.md)

## ç®€ä»‹

æ¬¢è¿æ¥åˆ° Diffusion æ¨¡å‹çš„é­”æ³•ä¸–ç•Œï¼DiffSynth-Studio æ˜¯ç”±[é­”æ­ç¤¾åŒº](https://www.modelscope.cn/)å›¢é˜Ÿå¼€å‘å’Œç»´æŠ¤çš„å¼€æº Diffusion æ¨¡å‹å¼•æ“ã€‚æˆ‘ä»¬æœŸæœ›ä»¥æ¡†æ¶å»ºè®¾å­µåŒ–æŠ€æœ¯åˆ›æ–°ï¼Œå‡èšå¼€æºç¤¾åŒºçš„åŠ›é‡ï¼Œæ¢ç´¢ç”Ÿæˆå¼æ¨¡å‹æŠ€æœ¯çš„è¾¹ç•Œï¼

DiffSynth ç›®å‰åŒ…æ‹¬ä¸¤ä¸ªå¼€æºé¡¹ç›®ï¼š
* [DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio): èšç„¦äºæ¿€è¿›çš„æŠ€æœ¯æ¢ç´¢ï¼Œé¢å‘å­¦æœ¯ç•Œï¼Œæä¾›æ›´å‰æ²¿çš„æ¨¡å‹èƒ½åŠ›æ”¯æŒã€‚
* [DiffSynth-Engine](https://github.com/modelscope/DiffSynth-Engine): èšç„¦äºç¨³å®šçš„æ¨¡å‹éƒ¨ç½²ï¼Œé¢å‘å·¥ä¸šç•Œï¼Œæä¾›æ›´é«˜çš„è®¡ç®—æ€§èƒ½ä¸æ›´ç¨³å®šçš„åŠŸèƒ½ã€‚

[DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio) ä¸ [DiffSynth-Engine](https://github.com/modelscope/DiffSynth-Engine) æ˜¯é­”æ­ç¤¾åŒº AIGC ä¸“åŒºçš„æ ¸å¿ƒå¼•æ“ï¼Œæ¬¢è¿ä½“éªŒæˆ‘ä»¬ç²¾å¿ƒæ‰“é€ çš„äº§å“åŒ–åŠŸèƒ½ï¼š

* é­”æ­ç¤¾åŒº AIGC ä¸“åŒº (é¢å‘ä¸­å›½ç”¨æˆ·): https://modelscope.cn/aigc/home
* ModelScope Civision (for global users): https://modelscope.ai/civision/home

> DiffSynth-Studio æ–‡æ¡£ï¼š[ä¸­æ–‡ç‰ˆ](/docs/zh/README.md)ã€[English version](/docs/en/README.md)

æˆ‘ä»¬ç›¸ä¿¡ï¼Œä¸€ä¸ªå®Œå–„çš„å¼€æºä»£ç æ¡†æ¶èƒ½å¤Ÿé™ä½æŠ€æœ¯æ¢ç´¢çš„é—¨æ§›ï¼Œæˆ‘ä»¬åŸºäºè¿™ä¸ªä»£ç åº“æå‡ºäº†ä¸å°‘[æœ‰æ„æ€çš„æŠ€æœ¯](#åˆ›æ–°æˆæœ)ã€‚æˆ–è®¸ä½ ä¹Ÿæœ‰è®¸å¤šå¤©é©¬è¡Œç©ºçš„æ„æƒ³ï¼Œå€ŸåŠ© DiffSynth-Studioï¼Œä½ å¯ä»¥å¿«é€Ÿå®ç°è¿™äº›æƒ³æ³•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä¸ºå¼€å‘è€…å‡†å¤‡äº†è¯¦ç»†çš„æ–‡æ¡£ï¼Œæˆ‘ä»¬å¸Œæœ›é€šè¿‡è¿™äº›æ–‡æ¡£ï¼Œå¸®åŠ©å¼€å‘è€…ç†è§£ Diffusion æ¨¡å‹çš„åŸç†ï¼Œæ›´æœŸå¾…ä¸ä½ ä¸€åŒæ‹“å±•æŠ€æœ¯çš„è¾¹ç•Œã€‚

## æ›´æ–°å†å²

> DiffSynth-Studio ç»å†äº†å¤§ç‰ˆæœ¬æ›´æ–°ï¼Œéƒ¨åˆ†æ—§åŠŸèƒ½å·²åœæ­¢ç»´æŠ¤ï¼Œå¦‚éœ€ä½¿ç”¨æ—§ç‰ˆåŠŸèƒ½ï¼Œè¯·åˆ‡æ¢åˆ°å¤§ç‰ˆæœ¬æ›´æ–°å‰çš„[æœ€åä¸€ä¸ªå†å²ç‰ˆæœ¬](https://github.com/modelscope/DiffSynth-Studio/tree/afd101f3452c9ecae0c87b79adfa2e22d65ffdc3)ã€‚

> ç›®å‰æœ¬é¡¹ç›®çš„å¼€å‘äººå‘˜æœ‰é™ï¼Œå¤§éƒ¨åˆ†å·¥ä½œç”± [Artiprocher](https://github.com/Artiprocher) è´Ÿè´£ï¼Œå› æ­¤æ–°åŠŸèƒ½çš„å¼€å‘è¿›å±•ä¼šæ¯”è¾ƒç¼“æ…¢ï¼Œissue çš„å›å¤å’Œè§£å†³é€Ÿåº¦æœ‰é™ï¼Œæˆ‘ä»¬å¯¹æ­¤æ„Ÿåˆ°éå¸¸æŠ±æ­‰ï¼Œè¯·å„ä½å¼€å‘è€…ç†è§£ã€‚

- **2026å¹´2æœˆ2æ—¥** Research Tutorial çš„ç¬¬ä¸€ç¯‡æ–‡æ¡£ä¸Šçº¿ï¼Œå¸¦ä½ ä»é›¶å¼€å§‹è®­ç»ƒä¸€ä¸ª 0.1B çš„å°å‹æ–‡ç”Ÿå›¾æ¨¡å‹ï¼Œè¯¦è§[æ–‡æ¡£](/docs/zh/Research_Tutorial/train_from_scratch.md)ã€[æ¨¡å‹](https://modelscope.cn/models/DiffSynth-Studio/AAAMyModel)ï¼Œæˆ‘ä»¬å¸Œæœ› DiffSynth-Studio èƒ½å¤Ÿæˆä¸ºä¸€ä¸ªæ›´å¼ºå¤§çš„ Diffusion æ¨¡å‹è®­ç»ƒæ¡†æ¶ã€‚

- **2026å¹´1æœˆ27æ—¥** [Z-Image](https://modelscope.cn/models/Tongyi-MAI/Z-Image) å‘å¸ƒï¼Œæˆ‘ä»¬çš„ [Z-Image-i2L](https://www.modelscope.cn/models/DiffSynth-Studio/Z-Image-i2L) æ¨¡å‹åŒæ­¥å‘å¸ƒï¼Œåœ¨[é­”æ­åˆ›ç©ºé—´](https://modelscope.cn/studios/DiffSynth-Studio/Z-Image-i2L)å¯ç›´æ¥ä½“éªŒï¼Œè¯¦è§[æ–‡æ¡£](/docs/zh/Model_Details/Z-Image.md)ã€‚

- **2026å¹´1æœˆ19æ—¥** æ–°å¢å¯¹ [FLUX.2-klein-4B](https://modelscope.cn/models/black-forest-labs/FLUX.2-klein-4B) å’Œ [FLUX.2-klein-9B](https://modelscope.cn/models/black-forest-labs/FLUX.2-klein-9B) æ¨¡å‹çš„æ”¯æŒï¼ŒåŒ…æ‹¬å®Œæ•´çš„è®­ç»ƒå’Œæ¨ç†åŠŸèƒ½ã€‚[æ–‡æ¡£](/docs/zh/Model_Details/FLUX2.md)å’Œ[ç¤ºä¾‹ä»£ç ](/examples/flux2/)ç°å·²å¯ç”¨ã€‚

- **2026å¹´1æœˆ12æ—¥** æˆ‘ä»¬è®­ç»ƒå¹¶å¼€æºäº†ä¸€ä¸ªæ–‡æœ¬å¼•å¯¼çš„å›¾å±‚æ‹†åˆ†æ¨¡å‹ï¼ˆ[æ¨¡å‹é“¾æ¥](https://modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Layered-Control)ï¼‰ï¼Œè¿™ä¸€æ¨¡å‹è¾“å…¥ä¸€å¼ å›¾ä¸ä¸€æ®µæ–‡æœ¬æè¿°ï¼Œæ¨¡å‹ä¼šå°†å›¾åƒä¸­ä¸æ–‡æœ¬æè¿°ç›¸å…³çš„å›¾å±‚æ‹†åˆ†å‡ºæ¥ã€‚æ›´å¤šç»†èŠ‚è¯·é˜…è¯»æˆ‘ä»¬çš„ blogï¼ˆ[ä¸­æ–‡ç‰ˆ](https://modelscope.cn/learn/4938)ã€[è‹±æ–‡ç‰ˆ](https://huggingface.co/blog/kelseye/qwen-image-layered-control)ï¼‰ã€‚

- **2025å¹´12æœˆ24æ—¥** æˆ‘ä»¬åŸºäº Qwen-Image-Edit-2511 è®­ç»ƒäº†ä¸€ä¸ª In-Context Editing LoRA æ¨¡å‹ï¼ˆ[æ¨¡å‹é“¾æ¥](https://modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Edit-2511-ICEdit-LoRA)ï¼‰ï¼Œè¿™ä¸ªæ¨¡å‹å¯ä»¥è¾“å…¥ä¸‰å¼ å›¾ï¼šå›¾Aã€å›¾Bã€å›¾Cï¼Œæ¨¡å‹ä¼šè‡ªè¡Œåˆ†æå›¾Aåˆ°å›¾Bçš„å˜åŒ–ï¼Œå¹¶å°†è¿™æ ·çš„å˜åŒ–åº”ç”¨åˆ°å›¾Cï¼Œç”Ÿæˆå›¾Dã€‚æ›´å¤šç»†èŠ‚è¯·é˜…è¯»æˆ‘ä»¬çš„ blogï¼ˆ[ä¸­æ–‡ç‰ˆ](https://mp.weixin.qq.com/s/41aEiN3lXKGCJs1-we4Q2g)ã€[è‹±æ–‡ç‰ˆ](https://huggingface.co/blog/kelseye/qwen-image-edit-2511-icedit-lora)ï¼‰ã€‚

- **2025å¹´12æœˆ9æ—¥** æˆ‘ä»¬åŸºäº DiffSynth-Studio 2.0 è®­ç»ƒäº†ä¸€ä¸ªç–¯ç‹‚çš„æ¨¡å‹ï¼š[Qwen-Image-i2L](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-i2L)ï¼ˆImage to LoRAï¼‰ã€‚è¿™ä¸€æ¨¡å‹ä»¥å›¾åƒä¸ºè¾“å…¥ï¼Œä»¥ LoRA ä¸ºè¾“å‡ºã€‚å°½ç®¡è¿™ä¸ªç‰ˆæœ¬çš„æ¨¡å‹åœ¨æ³›åŒ–èƒ½åŠ›ã€ç»†èŠ‚ä¿æŒèƒ½åŠ›ç­‰æ–¹é¢è¿˜æœ‰å¾ˆå¤§æ”¹è¿›ç©ºé—´ï¼Œæˆ‘ä»¬å°†è¿™äº›æ¨¡å‹å¼€æºï¼Œä»¥å¯å‘æ›´å¤šåˆ›æ–°æ€§çš„ç ”ç©¶å·¥ä½œã€‚æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„ [blog](https://huggingface.co/blog/kelseye/qwen-image-i2l)ã€‚

- **2025å¹´12æœˆ4æ—¥** DiffSynth-Studio 2.0 å‘å¸ƒï¼ä¼—å¤šæ–°åŠŸèƒ½ä¸Šçº¿
  - [æ–‡æ¡£](/docs/zh/README.md)ä¸Šçº¿ï¼šæˆ‘ä»¬çš„æ–‡æ¡£è¿˜åœ¨æŒç»­ä¼˜åŒ–æ›´æ–°ä¸­
  - [æ˜¾å­˜ç®¡ç†](/docs/zh/Pipeline_Usage/VRAM_management.md)æ¨¡å—å‡çº§ï¼Œæ”¯æŒ Layer çº§åˆ«çš„ Disk Offloadï¼ŒåŒæ—¶é‡Šæ”¾å†…å­˜ä¸æ˜¾å­˜
  - æ–°æ¨¡å‹æ”¯æŒ
    - Z-Image Turbo: [æ¨¡å‹](https://www.modelscope.ai/models/Tongyi-MAI/Z-Image-Turbo)ã€[æ–‡æ¡£](/docs/zh/Model_Details/Z-Image.md)ã€[ä»£ç ](/examples/z_image/)
    - FLUX.2-dev: [æ¨¡å‹](https://www.modelscope.cn/models/black-forest-labs/FLUX.2-dev)ã€[æ–‡æ¡£](/docs/zh/Model_Details/FLUX2.md)ã€[ä»£ç ](/examples/flux2/)
  - è®­ç»ƒæ¡†æ¶å‡çº§
    - [æ‹†åˆ†è®­ç»ƒ](/docs/zh/Training/Split_Training.md)ï¼šæ”¯æŒè‡ªåŠ¨åŒ–åœ°å°†è®­ç»ƒè¿‡ç¨‹æ‹†åˆ†ä¸ºæ•°æ®å¤„ç†å’Œè®­ç»ƒä¸¤é˜¶æ®µï¼ˆå³ä½¿è®­ç»ƒçš„æ˜¯ ControlNet æˆ–å…¶ä»–ä»»æ„æ¨¡å‹ï¼‰ï¼Œåœ¨æ•°æ®å¤„ç†é˜¶æ®µè¿›è¡Œæ–‡æœ¬ç¼–ç ã€VAE ç¼–ç ç­‰ä¸éœ€è¦æ¢¯åº¦å›ä¼ çš„è®¡ç®—ï¼Œåœ¨è®­ç»ƒé˜¶æ®µå¤„ç†å…¶ä»–è®¡ç®—ã€‚é€Ÿåº¦æ›´å¿«ï¼Œæ˜¾å­˜éœ€æ±‚æ›´å°‘ã€‚
    - [å·®åˆ† LoRA è®­ç»ƒ](/docs/zh/Training/Differential_LoRA.md)ï¼šè¿™æ˜¯æˆ‘ä»¬æ›¾åœ¨ [ArtAug](https://www.modelscope.cn/models/DiffSynth-Studio/ArtAug-lora-FLUX.1dev-v1) ä¸­ä½¿ç”¨çš„è®­ç»ƒæŠ€æœ¯ï¼Œç›®å‰å·²å¯ç”¨äºä»»æ„æ¨¡å‹çš„ LoRA è®­ç»ƒã€‚
    - [FP8 è®­ç»ƒ](/docs/zh/Training/FP8_Precision.md)ï¼šFP8 åœ¨è®­ç»ƒä¸­æ”¯æŒåº”ç”¨åˆ°ä»»æ„éè®­ç»ƒæ¨¡å‹ï¼Œå³æ¢¯åº¦å…³é—­æˆ–è€…æ¢¯åº¦ä»…å½±å“ LoRA æƒé‡çš„æ¨¡å‹ã€‚

<details>
<summary>æ›´å¤š</summary>

- **2025å¹´11æœˆ4æ—¥** æ”¯æŒäº† [ByteDance/Video-As-Prompt-Wan2.1-14B](https://modelscope.cn/models/ByteDance/Video-As-Prompt-Wan2.1-14B) æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäº Wan 2.1 è®­ç»ƒï¼Œæ”¯æŒæ ¹æ®å‚è€ƒè§†é¢‘ç”Ÿæˆç›¸åº”çš„åŠ¨ä½œã€‚

- **2025å¹´10æœˆ30æ—¥** æ”¯æŒäº† [meituan-longcat/LongCat-Video](https://www.modelscope.cn/models/meituan-longcat/LongCat-Video) æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ”¯æŒæ–‡ç”Ÿè§†é¢‘ã€å›¾ç”Ÿè§†é¢‘ã€è§†é¢‘ç»­å†™ã€‚è¿™ä¸ªæ¨¡å‹åœ¨æœ¬é¡¹ç›®ä¸­æ²¿ç”¨ Wan çš„æ¡†æ¶è¿›è¡Œæ¨ç†å’Œè®­ç»ƒã€‚

- **2025å¹´10æœˆ27æ—¥** æ”¯æŒäº† [krea/krea-realtime-video](https://www.modelscope.cn/models/krea/krea-realtime-video) æ¨¡å‹ï¼ŒWan æ¨¡å‹ç”Ÿæ€å†æ·»ä¸€å‘˜ã€‚

- **2025å¹´9æœˆ23æ—¥** [DiffSynth-Studio/Qwen-Image-EliGen-Poster](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-EliGen-Poster) å‘å¸ƒï¼æœ¬æ¨¡å‹ç”±æˆ‘ä»¬ä¸æ·˜å¤©ä½“éªŒè®¾è®¡å›¢é˜Ÿè”åˆç ”å‘å¹¶å¼€æºã€‚æ¨¡å‹åŸºäº Qwen-Image æ„å»ºï¼Œä¸“ä¸ºç”µå•†æµ·æŠ¥åœºæ™¯è®¾è®¡ï¼Œæ”¯æŒç²¾ç¡®çš„åˆ†åŒºå¸ƒå±€æ§åˆ¶ã€‚ è¯·å‚è€ƒ[æˆ‘ä»¬çš„ç¤ºä¾‹ä»£ç ](./examples/qwen_image/model_inference/Qwen-Image-EliGen-Poster.py)ã€‚

- **2025å¹´9æœˆ9æ—¥** æˆ‘ä»¬çš„è®­ç»ƒæ¡†æ¶æ”¯æŒäº†å¤šç§è®­ç»ƒæ¨¡å¼ï¼Œç›®å‰å·²é€‚é… Qwen-Imageï¼Œé™¤æ ‡å‡† SFT è®­ç»ƒæ¨¡å¼å¤–ï¼Œå·²æ”¯æŒ Direct Distillï¼Œè¯·å‚è€ƒ[æˆ‘ä»¬çš„ç¤ºä¾‹ä»£ç ](./examples/qwen_image/model_training/lora/Qwen-Image-Distill-LoRA.sh)ã€‚è¿™é¡¹åŠŸèƒ½æ˜¯å®éªŒæ€§çš„ï¼Œæˆ‘ä»¬å°†ä¼šç»§ç»­å®Œå–„å·²æ”¯æŒæ›´å…¨é¢çš„æ¨¡å‹è®­ç»ƒåŠŸèƒ½ã€‚

- **2025å¹´8æœˆ28æ—¥** æˆ‘ä»¬æ”¯æŒäº†Wan2.2-S2Vï¼Œä¸€ä¸ªéŸ³é¢‘é©±åŠ¨çš„ç”µå½±çº§è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚è¯·å‚è§[./examples/wanvideo/](./examples/wanvideo/)ã€‚

- **2025å¹´8æœˆ21æ—¥** [DiffSynth-Studio/Qwen-Image-EliGen-V2](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-EliGen-V2) å‘å¸ƒï¼ç›¸æ¯”äº V1 ç‰ˆæœ¬ï¼Œè®­ç»ƒæ•°æ®é›†å˜ä¸º [Qwen-Image-Self-Generated-Dataset](https://www.modelscope.cn/datasets/DiffSynth-Studio/Qwen-Image-Self-Generated-Dataset)ï¼Œå› æ­¤ï¼Œç”Ÿæˆçš„å›¾åƒæ›´ç¬¦åˆ Qwen-Image æœ¬èº«çš„å›¾åƒåˆ†å¸ƒå’Œé£æ ¼ã€‚ è¯·å‚è€ƒ[æˆ‘ä»¬çš„ç¤ºä¾‹ä»£ç ](./examples/qwen_image/model_inference_low_vram/Qwen-Image-EliGen-V2.py)ã€‚

- **2025å¹´8æœˆ21æ—¥** æˆ‘ä»¬å¼€æºäº† [DiffSynth-Studio/Qwen-Image-In-Context-Control-Union](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-In-Context-Control-Union) ç»“æ„æ§åˆ¶ LoRA æ¨¡å‹ï¼Œé‡‡ç”¨ In Context çš„æŠ€æœ¯è·¯çº¿ï¼Œæ”¯æŒå¤šç§ç±»åˆ«çš„ç»“æ„æ§åˆ¶æ¡ä»¶ï¼ŒåŒ…æ‹¬ canny, depth, lineart, softedge, normal, openposeã€‚ è¯·å‚è€ƒ[æˆ‘ä»¬çš„ç¤ºä¾‹ä»£ç ](./examples/qwen_image/model_inference/Qwen-Image-In-Context-Control-Union.py)ã€‚

- **2025å¹´8æœˆ20æ—¥** æˆ‘ä»¬å¼€æºäº† [DiffSynth-Studio/Qwen-Image-Edit-Lowres-Fix](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Edit-Lowres-Fix) æ¨¡å‹ï¼Œæå‡äº† Qwen-Image-Edit å¯¹ä½åˆ†è¾¨ç‡å›¾åƒè¾“å…¥çš„ç¼–è¾‘æ•ˆæœã€‚è¯·å‚è€ƒ[æˆ‘ä»¬çš„ç¤ºä¾‹ä»£ç ](./examples/qwen_image/model_inference/Qwen-Image-Edit-Lowres-Fix.py)

- **2025å¹´8æœˆ19æ—¥** ğŸ”¥ Qwen-Image-Edit å¼€æºï¼Œæ¬¢è¿å›¾åƒç¼–è¾‘æ¨¡å‹æ–°æˆå‘˜ï¼

- **2025å¹´8æœˆ18æ—¥** æˆ‘ä»¬è®­ç»ƒå¹¶å¼€æºäº† Qwen-Image çš„å›¾åƒé‡ç»˜ ControlNet æ¨¡å‹ [DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Inpaint](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Inpaint)ï¼Œæ¨¡å‹ç»“æ„é‡‡ç”¨äº†è½»é‡åŒ–çš„è®¾è®¡ï¼Œè¯·å‚è€ƒ[æˆ‘ä»¬çš„ç¤ºä¾‹ä»£ç ](./examples/qwen_image/model_inference/Qwen-Image-Blockwise-ControlNet-Inpaint.py)ã€‚

- **2025å¹´8æœˆ15æ—¥** æˆ‘ä»¬å¼€æºäº† [Qwen-Image-Self-Generated-Dataset](https://www.modelscope.cn/datasets/DiffSynth-Studio/Qwen-Image-Self-Generated-Dataset) æ•°æ®é›†ã€‚è¿™æ˜¯ä¸€ä¸ªä½¿ç”¨ Qwen-Image æ¨¡å‹ç”Ÿæˆçš„å›¾åƒæ•°æ®é›†ï¼Œå…±åŒ…å« 160,000 å¼ `1024 x 1024`å›¾åƒã€‚å®ƒåŒ…æ‹¬é€šç”¨ã€è‹±æ–‡æ–‡æœ¬æ¸²æŸ“å’Œä¸­æ–‡æ–‡æœ¬æ¸²æŸ“å­é›†ã€‚æˆ‘ä»¬ä¸ºæ¯å¼ å›¾åƒæä¾›äº†å›¾åƒæè¿°ã€å®ä½“å’Œç»“æ„æ§åˆ¶å›¾åƒçš„æ ‡æ³¨ã€‚å¼€å‘è€…å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ•°æ®é›†æ¥è®­ç»ƒ Qwen-Image æ¨¡å‹çš„ ControlNet å’Œ EliGen ç­‰æ¨¡å‹ï¼Œæˆ‘ä»¬æ—¨åœ¨é€šè¿‡å¼€æºæ¨åŠ¨æŠ€æœ¯å‘å±•ï¼

- **2025å¹´8æœˆ13æ—¥** æˆ‘ä»¬è®­ç»ƒå¹¶å¼€æºäº† Qwen-Image çš„ ControlNet æ¨¡å‹ [DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Depth](https://modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Depth)ï¼Œæ¨¡å‹ç»“æ„é‡‡ç”¨äº†è½»é‡åŒ–çš„è®¾è®¡ï¼Œè¯·å‚è€ƒ[æˆ‘ä»¬çš„ç¤ºä¾‹ä»£ç ](./examples/qwen_image/model_inference/Qwen-Image-Blockwise-ControlNet-Depth.py)ã€‚

- **2025å¹´8æœˆ12æ—¥** æˆ‘ä»¬è®­ç»ƒå¹¶å¼€æºäº† Qwen-Image çš„ ControlNet æ¨¡å‹ [DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Canny](https://modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Canny)ï¼Œæ¨¡å‹ç»“æ„é‡‡ç”¨äº†è½»é‡åŒ–çš„è®¾è®¡ï¼Œè¯·å‚è€ƒ[æˆ‘ä»¬çš„ç¤ºä¾‹ä»£ç ](./examples/qwen_image/model_inference/Qwen-Image-Blockwise-ControlNet-Canny.py)ã€‚

- **2025å¹´8æœˆ11æ—¥** æˆ‘ä»¬å¼€æºäº† Qwen-Image çš„è’¸é¦åŠ é€Ÿæ¨¡å‹ [DiffSynth-Studio/Qwen-Image-Distill-LoRA](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Distill-LoRA)ï¼Œæ²¿ç”¨äº†ä¸ [DiffSynth-Studio/Qwen-Image-Distill-Full](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Distill-Full) ç›¸åŒçš„è®­ç»ƒæµç¨‹ï¼Œä½†æ¨¡å‹ç»“æ„ä¿®æ”¹ä¸ºäº† LoRAï¼Œå› æ­¤èƒ½å¤Ÿæ›´å¥½åœ°ä¸å…¶ä»–å¼€æºç”Ÿæ€æ¨¡å‹å…¼å®¹ã€‚

- **2025å¹´8æœˆ7æ—¥** æˆ‘ä»¬å¼€æºäº† Qwen-Image çš„å®ä½“æ§åˆ¶ LoRA æ¨¡å‹ [DiffSynth-Studio/Qwen-Image-EliGen](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-EliGen)ã€‚Qwen-Image-EliGen èƒ½å¤Ÿå®ç°å®ä½“çº§å¯æ§çš„æ–‡ç”Ÿå›¾ã€‚æŠ€æœ¯ç»†èŠ‚è¯·å‚è§[è®ºæ–‡](https://arxiv.org/abs/2501.01097)ã€‚è®­ç»ƒæ•°æ®é›†ï¼š[EliGenTrainSet](https://www.modelscope.cn/datasets/DiffSynth-Studio/EliGenTrainSet)ã€‚

- **2025å¹´8æœˆ5æ—¥** æˆ‘ä»¬å¼€æºäº† Qwen-Image çš„è’¸é¦åŠ é€Ÿæ¨¡å‹ [DiffSynth-Studio/Qwen-Image-Distill-Full](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Distill-Full)ï¼Œå®ç°äº†çº¦ 5 å€åŠ é€Ÿã€‚

- **2025å¹´8æœˆ4æ—¥** ğŸ”¥ Qwen-Image å¼€æºï¼Œæ¬¢è¿å›¾åƒç”Ÿæˆæ¨¡å‹å®¶æ—æ–°æˆå‘˜ï¼

- **2025å¹´8æœˆ1æ—¥** [FLUX.1-Krea-dev](https://www.modelscope.cn/models/black-forest-labs/FLUX.1-Krea-dev) å¼€æºï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“æ³¨äºç¾å­¦æ‘„å½±çš„æ–‡ç”Ÿå›¾æ¨¡å‹ã€‚æˆ‘ä»¬ç¬¬ä¸€æ—¶é—´æä¾›äº†å…¨æ–¹ä½æ”¯æŒï¼ŒåŒ…æ‹¬ä½æ˜¾å­˜é€å±‚ offloadã€LoRA è®­ç»ƒã€å…¨é‡è®­ç»ƒã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒ [./examples/flux/](./examples/flux/)ã€‚

- **2025å¹´7æœˆ28æ—¥** Wan 2.2 å¼€æºï¼Œæˆ‘ä»¬ç¬¬ä¸€æ—¶é—´æä¾›äº†å…¨æ–¹ä½æ”¯æŒï¼ŒåŒ…æ‹¬ä½æ˜¾å­˜é€å±‚ offloadã€FP8 é‡åŒ–ã€åºåˆ—å¹¶è¡Œã€LoRA è®­ç»ƒã€å…¨é‡è®­ç»ƒã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒ [./examples/wanvideo/](./examples/wanvideo/)ã€‚

- **2025å¹´7æœˆ11æ—¥** æˆ‘ä»¬æå‡º Nexus-Genï¼Œä¸€ä¸ªå°†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­è¨€æ¨ç†èƒ½åŠ›ä¸æ‰©æ•£æ¨¡å‹çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ç›¸ç»“åˆçš„ç»Ÿä¸€æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ”¯æŒæ— ç¼çš„å›¾åƒç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ã€‚
  - è®ºæ–‡: [Nexus-Gen: Unified Image Understanding, Generation, and Editing via Prefilled Autoregression in Shared Embedding Space](https://arxiv.org/pdf/2504.21356)
  - Github ä»“åº“: https://github.com/modelscope/Nexus-Gen
  - æ¨¡å‹: [ModelScope](https://www.modelscope.cn/models/DiffSynth-Studio/Nexus-GenV2), [HuggingFace](https://huggingface.co/modelscope/Nexus-GenV2)
  - è®­ç»ƒæ•°æ®é›†: [ModelScope Dataset](https://www.modelscope.cn/datasets/DiffSynth-Studio/Nexus-Gen-Training-Dataset)
  - åœ¨çº¿ä½“éªŒ: [ModelScope Nexus-Gen Studio](https://www.modelscope.cn/studios/DiffSynth-Studio/Nexus-Gen)

- **2025å¹´6æœˆ15æ—¥** ModelScope å®˜æ–¹è¯„æµ‹æ¡†æ¶ [EvalScope](https://github.com/modelscope/evalscope) ç°å·²æ”¯æŒæ–‡ç”Ÿå›¾ç”Ÿæˆè¯„æµ‹ã€‚è¯·å‚è€ƒ[æœ€ä½³å®è·µ](https://evalscope.readthedocs.io/zh-cn/latest/best_practice/t2i_eval.html)æŒ‡å—è¿›è¡Œå°è¯•ã€‚

- **2025å¹´3æœˆ25æ—¥** æˆ‘ä»¬çš„æ–°å¼€æºé¡¹ç›® [DiffSynth-Engine](https://github.com/modelscope/DiffSynth-Engine) ç°å·²å¼€æºï¼ä¸“æ³¨äºç¨³å®šçš„æ¨¡å‹éƒ¨ç½²ï¼Œé¢å‘å·¥ä¸šç•Œï¼Œæä¾›æ›´å¥½çš„å·¥ç¨‹æ”¯æŒã€æ›´é«˜çš„è®¡ç®—æ€§èƒ½å’Œæ›´ç¨³å®šçš„åŠŸèƒ½ã€‚

- **2025å¹´3æœˆ31æ—¥** æˆ‘ä»¬æ”¯æŒ InfiniteYouï¼Œä¸€ç§ç”¨äº FLUX çš„äººè„¸ç‰¹å¾ä¿ç•™æ–¹æ³•ã€‚æ›´å¤šç»†èŠ‚è¯·å‚è€ƒ [./examples/InfiniteYou/](./examples/InfiniteYou/)ã€‚

- **2025å¹´3æœˆ13æ—¥** æˆ‘ä»¬æ”¯æŒ HunyuanVideo-I2Vï¼Œå³è…¾è®¯å¼€æºçš„ HunyuanVideo çš„å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆç‰ˆæœ¬ã€‚æ›´å¤šç»†èŠ‚è¯·å‚è€ƒ [./examples/HunyuanVideo/](./examples/HunyuanVideo/)ã€‚

- **2025å¹´2æœˆ25æ—¥** æˆ‘ä»¬æ”¯æŒ Wan-Videoï¼Œè¿™æ˜¯é˜¿é‡Œå·´å·´å¼€æºçš„ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„è§†é¢‘åˆæˆæ¨¡å‹ã€‚è¯¦è§ [./examples/wanvideo/](./examples/wanvideo/)ã€‚

- **2025å¹´2æœˆ17æ—¥** æˆ‘ä»¬æ”¯æŒ [StepVideo](https://modelscope.cn/models/stepfun-ai/stepvideo-t2v/summary)ï¼å…ˆè¿›çš„è§†é¢‘åˆæˆæ¨¡å‹ï¼è¯¦è§ [./examples/stepvideo](./examples/stepvideo/)ã€‚

- **2024å¹´12æœˆ31æ—¥** æˆ‘ä»¬æå‡º EliGenï¼Œä¸€ç§ç”¨äºç²¾ç¡®å®ä½“çº§åˆ«æ§åˆ¶çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æ–°æ¡†æ¶ï¼Œå¹¶è¾…ä»¥ä¿®å¤èåˆç®¡é“ï¼Œå°†å…¶èƒ½åŠ›æ‰©å±•åˆ°å›¾åƒä¿®å¤ä»»åŠ¡ã€‚EliGen å¯ä»¥æ— ç¼é›†æˆç°æœ‰çš„ç¤¾åŒºæ¨¡å‹ï¼Œå¦‚ IP-Adapter å’Œ In-Context LoRAï¼Œæå‡å…¶é€šç”¨æ€§ã€‚æ›´å¤šè¯¦æƒ…ï¼Œè¯·è§ [./examples/EntityControl](./examples/EntityControl/)ã€‚
  - è®ºæ–‡: [EliGen: Entity-Level Controlled Image Generation with Regional Attention](https://arxiv.org/abs/2501.01097)
  - æ¨¡å‹: [ModelScope](https://www.modelscope.cn/models/DiffSynth-Studio/Eligen), [HuggingFace](https://huggingface.co/modelscope/EliGen)
  - åœ¨çº¿ä½“éªŒ: [ModelScope EliGen Studio](https://www.modelscope.cn/studios/DiffSynth-Studio/EliGen)
  - è®­ç»ƒæ•°æ®é›†: [EliGen Train Set](https://www.modelscope.cn/datasets/DiffSynth-Studio/EliGenTrainSet)

- **2024å¹´12æœˆ19æ—¥** æˆ‘ä»¬ä¸º HunyuanVideo å®ç°äº†é«˜çº§æ˜¾å­˜ç®¡ç†ï¼Œä½¿å¾—åœ¨ 24GB æ˜¾å­˜ä¸‹å¯ä»¥ç”Ÿæˆåˆ†è¾¨ç‡ä¸º 129x720x1280 çš„è§†é¢‘ï¼Œæˆ–åœ¨ä»… 6GB æ˜¾å­˜ä¸‹ç”Ÿæˆåˆ†è¾¨ç‡ä¸º 129x512x384 çš„è§†é¢‘ã€‚æ›´å¤šç»†èŠ‚è¯·å‚è€ƒ [./examples/HunyuanVideo/](./examples/HunyuanVideo/)ã€‚

- **2024å¹´12æœˆ18æ—¥** æˆ‘ä»¬æå‡º ArtAugï¼Œä¸€ç§é€šè¿‡åˆæˆ-ç†è§£äº¤äº’æ¥æ”¹è¿›æ–‡ç”Ÿå›¾æ¨¡å‹çš„æ–¹æ³•ã€‚æˆ‘ä»¬ä»¥ LoRA æ ¼å¼ä¸º FLUX.1-dev è®­ç»ƒäº†ä¸€ä¸ª ArtAug å¢å¼ºæ¨¡å—ã€‚è¯¥æ¨¡å‹å°† Qwen2-VL-72B çš„ç¾å­¦ç†è§£èå…¥ FLUX.1-devï¼Œä»è€Œæå‡äº†ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚
  - è®ºæ–‡: https://arxiv.org/abs/2412.12888
  - ç¤ºä¾‹: https://github.com/modelscope/DiffSynth-Studio/tree/main/examples/ArtAug
  - æ¨¡å‹: [ModelScope](https://www.modelscope.cn/models/DiffSynth-Studio/ArtAug-lora-FLUX.1dev-v1), [HuggingFace](https://huggingface.co/ECNU-CILab/ArtAug-lora-FLUX.1dev-v1)
  - æ¼”ç¤º: [ModelScope](https://modelscope.cn/aigc/imageGeneration?tab=advanced&versionId=7228&modelType=LoRA&sdVersion=FLUX_1&modelUrl=modelscope%3A%2F%2FDiffSynth-Studio%2FArtAug-lora-FLUX.1dev-v1%3Frevision%3Dv1.0), HuggingFace (å³å°†ä¸Šçº¿)

- **2024å¹´10æœˆ25æ—¥** æˆ‘ä»¬æä¾›äº†å¹¿æ³›çš„ FLUX ControlNet æ”¯æŒã€‚è¯¥é¡¹ç›®æ”¯æŒè®¸å¤šä¸åŒçš„ ControlNet æ¨¡å‹ï¼Œå¹¶ä¸”å¯ä»¥è‡ªç”±ç»„åˆï¼Œå³ä½¿å®ƒä»¬çš„ç»“æ„ä¸åŒã€‚æ­¤å¤–ï¼ŒControlNet æ¨¡å‹å…¼å®¹é«˜åˆ†è¾¨ç‡ä¼˜åŒ–å’Œåˆ†åŒºæ§åˆ¶æŠ€æœ¯ï¼Œèƒ½å¤Ÿå®ç°éå¸¸å¼ºå¤§çš„å¯æ§å›¾åƒç”Ÿæˆã€‚è¯¦è§ [`./examples/ControlNet/`](./examples/ControlNet/)ã€‚

- **2024å¹´10æœˆ8æ—¥** æˆ‘ä»¬å‘å¸ƒäº†åŸºäº CogVideoX-5B å’Œ ExVideo çš„æ‰©å±• LoRAã€‚æ‚¨å¯ä»¥ä» [ModelScope](https://modelscope.cn/models/ECNU-CILab/ExVideo-CogVideoX-LoRA-129f-v1) æˆ– [HuggingFace](https://huggingface.co/ECNU-CILab/ExVideo-CogVideoX-LoRA-129f-v1) ä¸‹è½½æ­¤æ¨¡å‹ã€‚

- **2024å¹´8æœˆ22æ—¥** æœ¬é¡¹ç›®ç°å·²æ”¯æŒ CogVideoX-5Bã€‚è¯¦è§ [æ­¤å¤„](/examples/video_synthesis/)ã€‚æˆ‘ä»¬ä¸ºè¿™ä¸ªæ–‡ç”Ÿè§†é¢‘æ¨¡å‹æä¾›äº†å‡ ä¸ªæœ‰è¶£çš„åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
  - æ–‡æœ¬åˆ°è§†é¢‘
  - è§†é¢‘ç¼–è¾‘
  - è‡ªæˆ‘è¶…åˆ†
  - è§†é¢‘æ’å¸§

- **2024å¹´8æœˆ22æ—¥** æˆ‘ä»¬å®ç°äº†ä¸€ä¸ªæœ‰è¶£çš„ç”»ç¬”åŠŸèƒ½ï¼Œæ”¯æŒæ‰€æœ‰æ–‡ç”Ÿå›¾æ¨¡å‹ã€‚ç°åœ¨ï¼Œæ‚¨å¯ä»¥åœ¨ AI çš„è¾…åŠ©ä¸‹ä½¿ç”¨ç”»ç¬”åˆ›ä½œæƒŠè‰³çš„å›¾åƒäº†ï¼
  - åœ¨æˆ‘ä»¬çš„ [WebUI](#usage-in-webui) ä¸­ä½¿ç”¨å®ƒã€‚

- **2024å¹´8æœˆ21æ—¥** DiffSynth-Studio ç°å·²æ”¯æŒ FLUXã€‚
  - å¯ç”¨ CFG å’Œé«˜åˆ†è¾¨ç‡ä¿®å¤ä»¥æå‡è§†è§‰è´¨é‡ã€‚è¯¦è§ [æ­¤å¤„](/examples/image_synthesis/README.md)
  - LoRAã€ControlNet å’Œå…¶ä»–é™„åŠ æ¨¡å‹å°†å¾ˆå¿«æ¨å‡ºã€‚

- **2024å¹´6æœˆ21æ—¥** æˆ‘ä»¬æå‡º ExVideoï¼Œä¸€ç§æ—¨åœ¨å¢å¼ºè§†é¢‘ç”Ÿæˆæ¨¡å‹èƒ½åŠ›çš„åè®­ç»ƒå¾®è°ƒæŠ€æœ¯ã€‚æˆ‘ä»¬å°† Stable Video Diffusion è¿›è¡Œäº†æ‰©å±•ï¼Œå®ç°äº†é•¿è¾¾ 128 å¸§çš„é•¿è§†é¢‘ç”Ÿæˆã€‚
  - [é¡¹ç›®é¡µé¢](https://ecnu-cilab.github.io/ExVideoProjectPage/)
  - æºä»£ç å·²åœ¨æ­¤ä»“åº“ä¸­å‘å¸ƒã€‚è¯¦è§ [`examples/ExVideo`](./examples/ExVideo/)ã€‚
  - æ¨¡å‹å·²å‘å¸ƒäº [HuggingFace](https://huggingface.co/ECNU-CILab/ExVideo-SVD-128f-v1) å’Œ [ModelScope](https://modelscope.cn/models/ECNU-CILab/ExVideo-SVD-128f-v1)ã€‚
  - æŠ€æœ¯æŠ¥å‘Šå·²å‘å¸ƒäº [arXiv](https://arxiv.org/abs/2406.14130)ã€‚
  - æ‚¨å¯ä»¥åœ¨æ­¤ [æ¼”ç¤º](https://huggingface.co/spaces/modelscope/ExVideo-SVD-128f-v1) ä¸­è¯•ç”¨ ExVideoï¼

- **2024å¹´6æœˆ13æ—¥** DiffSynth Studio å·²è¿ç§»è‡³ ModelScopeã€‚å¼€å‘å›¢é˜Ÿä¹Ÿä»â€œæˆ‘â€è½¬å˜ä¸ºâ€œæˆ‘ä»¬â€ã€‚å½“ç„¶ï¼Œæˆ‘ä»ä¼šå‚ä¸åç»­çš„å¼€å‘å’Œç»´æŠ¤å·¥ä½œã€‚

- **2024å¹´1æœˆ29æ—¥** æˆ‘ä»¬æå‡º Diffutoonï¼Œè¿™æ˜¯ä¸€ä¸ªå‡ºè‰²çš„å¡é€šç€è‰²è§£å†³æ–¹æ¡ˆã€‚
  - [é¡¹ç›®é¡µé¢](https://ecnu-cilab.github.io/DiffutoonProjectPage/)
  - æºä»£ç å·²åœ¨æ­¤é¡¹ç›®ä¸­å‘å¸ƒã€‚
  - æŠ€æœ¯æŠ¥å‘Šï¼ˆIJCAI 2024ï¼‰å·²å‘å¸ƒäº [arXiv](https://arxiv.org/abs/2401.16224)ã€‚

- **2023å¹´12æœˆ8æ—¥** æˆ‘ä»¬å†³å®šå¯åŠ¨ä¸€ä¸ªæ–°é¡¹ç›®ï¼Œæ—¨åœ¨é‡Šæ”¾æ‰©æ•£æ¨¡å‹çš„æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨è§†é¢‘åˆæˆæ–¹é¢ã€‚è¯¥é¡¹ç›®çš„å¼€å‘å·¥ä½œæ­£å¼å¼€å§‹ã€‚

- **2023å¹´11æœˆ15æ—¥** æˆ‘ä»¬æå‡º FastBlendï¼Œä¸€ç§å¼ºå¤§çš„è§†é¢‘å»é—ªçƒç®—æ³•ã€‚
  - sd-webui æ‰©å±•å·²å‘å¸ƒäº [GitHub](https://github.com/Artiprocher/sd-webui-fastblend)ã€‚
  - æ¼”ç¤ºè§†é¢‘å·²åœ¨ Bilibili ä¸Šå±•ç¤ºï¼ŒåŒ…å«ä¸‰ä¸ªä»»åŠ¡ï¼š
    - [è§†é¢‘å»é—ªçƒ](https://www.bilibili.com/video/BV1d94y1W7PE)
    - [è§†é¢‘æ’å¸§](https://www.bilibili.com/video/BV1Lw411m71p)
    - [å›¾åƒé©±åŠ¨çš„è§†é¢‘æ¸²æŸ“](https://www.bilibili.com/video/BV1RB4y1Z7LF)
  - æŠ€æœ¯æŠ¥å‘Šå·²å‘å¸ƒäº [arXiv](https://arxiv.org/abs/2311.09265)ã€‚
  - å…¶ä»–ç”¨æˆ·å¼€å‘çš„éå®˜æ–¹ ComfyUI æ‰©å±•å·²å‘å¸ƒäº [GitHub](https://github.com/AInseven/ComfyUI-fastblend)ã€‚

- **2023å¹´10æœˆ1æ—¥** æˆ‘ä»¬å‘å¸ƒäº†è¯¥é¡¹ç›®çš„æ—©æœŸç‰ˆæœ¬ï¼Œåä¸º FastSDXLã€‚è¿™æ˜¯æ„å»ºä¸€ä¸ªæ‰©æ•£å¼•æ“çš„åˆæ­¥å°è¯•ã€‚
  - æºä»£ç å·²å‘å¸ƒäº [GitHub](https://github.com/Artiprocher/FastSDXL)ã€‚
  - FastSDXL åŒ…å«ä¸€ä¸ªå¯è®­ç»ƒçš„ OLSS è°ƒåº¦å™¨ï¼Œä»¥æé«˜æ•ˆç‡ã€‚
    - OLSS çš„åŸå§‹ä»“åº“ä½äº [æ­¤å¤„](https://github.com/alibaba/EasyNLP/tree/master/diffusion/olss_scheduler)ã€‚
    - æŠ€æœ¯æŠ¥å‘Šï¼ˆCIKM 2023ï¼‰å·²å‘å¸ƒäº [arXiv](https://arxiv.org/abs/2305.14677)ã€‚
    - æ¼”ç¤ºè§†é¢‘å·²å‘å¸ƒäº [Bilibili](https://www.bilibili.com/video/BV1w8411y7uj)ã€‚
    - ç”±äº OLSS éœ€è¦é¢å¤–è®­ç»ƒï¼Œæˆ‘ä»¬æœªåœ¨æœ¬é¡¹ç›®ä¸­å®ç°å®ƒã€‚

- **2023å¹´8æœˆ29æ—¥** æˆ‘ä»¬æå‡º DiffSynthï¼Œä¸€ä¸ªè§†é¢‘åˆæˆæ¡†æ¶ã€‚
  - [é¡¹ç›®é¡µé¢](https://ecnu-cilab.github.io/DiffSynth.github.io/)ã€‚
  - æºä»£ç å·²å‘å¸ƒåœ¨ [EasyNLP](https://github.com/alibaba/EasyNLP/tree/master/diffusion/DiffSynth)ã€‚
  - æŠ€æœ¯æŠ¥å‘Šï¼ˆECML PKDD 2024ï¼‰å·²å‘å¸ƒäº [arXiv](https://arxiv.org/abs/2308.03463)ã€‚

</details>

## å®‰è£…

ä»æºç å®‰è£…ï¼ˆæ¨èï¼‰ï¼š

```
git clone https://github.com/modelscope/DiffSynth-Studio.git
cd DiffSynth-Studio
pip install -e .
```

æ›´å¤šå®‰è£…æ–¹å¼ï¼Œä»¥åŠé NVIDIA GPU çš„å®‰è£…ï¼Œè¯·å‚è€ƒ[å®‰è£…æ–‡æ¡£](/docs/zh/Pipeline_Usage/Setup.md)ã€‚

</details>

## åŸºç¡€æ¡†æ¶

DiffSynth-Studio ä¸ºä¸»æµ Diffusion æ¨¡å‹ï¼ˆåŒ…æ‹¬ FLUXã€Wan ç­‰ï¼‰é‡æ–°è®¾è®¡äº†æ¨ç†å’Œè®­ç»ƒæµæ°´çº¿ï¼Œèƒ½å¤Ÿå®ç°é«˜æ•ˆçš„æ˜¾å­˜ç®¡ç†ã€çµæ´»çš„æ¨¡å‹è®­ç»ƒã€‚

<details>
<summary>ç¯å¢ƒå˜é‡é…ç½®</summary>

> åœ¨è¿›è¡Œæ¨¡å‹æ¨ç†å’Œè®­ç»ƒå‰ï¼Œå¯é€šè¿‡[ç¯å¢ƒå˜é‡](/docs/zh/Pipeline_Usage/Environment_Variables.md)é…ç½®æ¨¡å‹ä¸‹è½½æºç­‰ã€‚
> 
> æœ¬é¡¹ç›®é»˜è®¤ä»é­”æ­ç¤¾åŒºä¸‹è½½æ¨¡å‹ã€‚å¯¹äºéä¸­å›½åŒºåŸŸçš„ç”¨æˆ·ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹é…ç½®ä»é­”æ­ç¤¾åŒºçš„å›½é™…ç«™ä¸‹è½½æ¨¡å‹ï¼š
> 
> ```python
> import os
> os.environ["MODELSCOPE_DOMAIN"] = "www.modelscope.ai"
> ```
> 
> å¦‚éœ€ä»å…¶ä»–ç«™ç‚¹ä¸‹è½½ï¼Œè¯·ä¿®æ”¹[ç¯å¢ƒå˜é‡ DIFFSYNTH_DOWNLOAD_SOURCE](/docs/zh/Pipeline_Usage/Environment_Variables.md#diffsynth_download_source)ã€‚

</details>

### å›¾åƒç”Ÿæˆæ¨¡å‹

![Image](https://github.com/user-attachments/assets/c01258e2-f251-441a-aa1e-ebb22f02594d)

#### Z-Imageï¼š[/docs/zh/Model_Details/Z-Image.md](/docs/zh/Model_Details/Z-Image.md)

<details>

<summary>å¿«é€Ÿå¼€å§‹</summary>

è¿è¡Œä»¥ä¸‹ä»£ç å¯ä»¥å¿«é€ŸåŠ è½½ [Tongyi-MAI/Z-Image-Turbo](https://www.modelscope.cn/models/Tongyi-MAI/Z-Image-Turbo) æ¨¡å‹å¹¶è¿›è¡Œæ¨ç†ã€‚FP8 ç²¾åº¦é‡åŒ–ä¼šå¯¼è‡´æ˜æ˜¾çš„å›¾åƒè´¨é‡åŠ£åŒ–ï¼Œå› æ­¤ä¸å»ºè®®åœ¨ Z-Image Turbo æ¨¡å‹ä¸Šå¼€å¯ä»»ä½•é‡åŒ–ï¼Œä»…å»ºè®®å¼€å¯ CPU Offloadï¼Œæœ€ä½ 8G æ˜¾å­˜å³å¯è¿è¡Œã€‚

```python
from diffsynth.pipelines.z_image import ZImagePipeline, ModelConfig
import torch

vram_config = {
    "offload_dtype": torch.bfloat16,
    "offload_device": "cpu",
    "onload_dtype": torch.bfloat16,
    "onload_device": "cpu",
    "preparing_dtype": torch.bfloat16,
    "preparing_device": "cuda",
    "computation_dtype": torch.bfloat16,
    "computation_device": "cuda",
}
pipe = ZImagePipeline.from_pretrained(
    torch_dtype=torch.bfloat16,
    device="cuda",
    model_configs=[
        ModelConfig(model_id="Tongyi-MAI/Z-Image-Turbo", origin_file_pattern="transformer/*.safetensors", **vram_config),
        ModelConfig(model_id="Tongyi-MAI/Z-Image-Turbo", origin_file_pattern="text_encoder/*.safetensors", **vram_config),
        ModelConfig(model_id="Tongyi-MAI/Z-Image-Turbo", origin_file_pattern="vae/diffusion_pytorch_model.safetensors", **vram_config),
    ],
    tokenizer_config=ModelConfig(model_id="Tongyi-MAI/Z-Image-Turbo", origin_file_pattern="tokenizer/"),
    vram_limit=torch.cuda.mem_get_info("cuda")[1] / (1024 ** 3) - 0.5,
)
prompt = "Young Chinese woman in red Hanfu, intricate embroidery. Impeccable makeup, red floral forehead pattern. Elaborate high bun, golden phoenix headdress, red flowers, beads. Holds round folding fan with lady, trees, bird. Neon lightning-bolt lamp (âš¡ï¸), bright yellow glow, above extended left palm. Soft-lit outdoor night background, silhouetted tiered pagoda (è¥¿å®‰å¤§é›å¡”), blurred colorful distant lights."
image = pipe(prompt=prompt, seed=42, rand_device="cuda")
image.save("image.jpg")
```

</details>

<details>

<summary>ç¤ºä¾‹ä»£ç </summary>

Z-Image çš„ç¤ºä¾‹ä»£ç ä½äºï¼š[/examples/z_image/](/examples/z_image/)

|æ¨¡å‹ ID|æ¨ç†|ä½æ˜¾å­˜æ¨ç†|å…¨é‡è®­ç»ƒ|å…¨é‡è®­ç»ƒåéªŒè¯|LoRA è®­ç»ƒ|LoRA è®­ç»ƒåéªŒè¯|
|-|-|-|-|-|-|-|
|[Tongyi-MAI/Z-Image](https://www.modelscope.cn/models/Tongyi-MAI/Z-Image)|[code](/examples/z_image/model_inference/Z-Image.py)|[code](/examples/z_image/model_inference_low_vram/Z-Image.py)|[code](/examples/z_image/model_training/full/Z-Image.sh)|[code](/examples/z_image/model_training/validate_full/Z-Image.py)|[code](/examples/z_image/model_training/lora/Z-Image.sh)|[code](/examples/z_image/model_training/validate_lora/Z-Image.py)|
|[DiffSynth-Studio/Z-Image-i2L](https://www.modelscope.cn/models/DiffSynth-Studio/Z-Image-i2L)|[code](/examples/z_image/model_inference/Z-Image-i2L.py)|[code](/examples/z_image/model_inference_low_vram/Z-Image-i2L.py)|-|-|-|-|
|[Tongyi-MAI/Z-Image-Turbo](https://www.modelscope.cn/models/Tongyi-MAI/Z-Image-Turbo)|[code](/examples/z_image/model_inference/Z-Image-Turbo.py)|[code](/examples/z_image/model_inference_low_vram/Z-Image-Turbo.py)|[code](/examples/z_image/model_training/full/Z-Image-Turbo.sh)|[code](/examples/z_image/model_training/validate_full/Z-Image-Turbo.py)|[code](/examples/z_image/model_training/lora/Z-Image-Turbo.sh)|[code](/examples/z_image/model_training/validate_lora/Z-Image-Turbo.py)|
|[PAI/Z-Image-Turbo-Fun-Controlnet-Union-2.1](https://www.modelscope.cn/models/PAI/Z-Image-Turbo-Fun-Controlnet-Union-2.1)|[code](/examples/z_image/model_inference/Z-Image-Turbo-Fun-Controlnet-Union-2.1.py)|[code](/examples/z_image/model_inference_low_vram/Z-Image-Turbo-Fun-Controlnet-Union-2.1.py)|[code](/examples/z_image/model_training/full/Z-Image-Turbo-Fun-Controlnet-Union-2.1.sh)|[code](/examples/z_image/model_training/validate_full/Z-Image-Turbo-Fun-Controlnet-Union-2.1.py)|[code](/examples/z_image/model_training/lora/Z-Image-Turbo-Fun-Controlnet-Union-2.1.sh)|[code](/examples/z_image/model_training/validate_lora/Z-Image-Turbo-Fun-Controlnet-Union-2.1.py)|
|[PAI/Z-Image-Turbo-Fun-Controlnet-Union-2.1-8steps](https://www.modelscope.cn/models/PAI/Z-Image-Turbo-Fun-Controlnet-Union-2.1)|[code](/examples/z_image/model_inference/Z-Image-Turbo-Fun-Controlnet-Union-2.1-8steps.py)|[code](/examples/z_image/model_inference_low_vram/Z-Image-Turbo-Fun-Controlnet-Union-2.1-8steps.py)|[code](/examples/z_image/model_training/full/Z-Image-Turbo-Fun-Controlnet-Union-2.1-8steps.sh)|[code](/examples/z_image/model_training/validate_full/Z-Image-Turbo-Fun-Controlnet-Union-2.1-8steps.py)|[code](/examples/z_image/model_training/lora/Z-Image-Turbo-Fun-Controlnet-Union-2.1-8steps.sh)|[code](/examples/z_image/model_training/validate_lora/Z-Image-Turbo-Fun-Controlnet-Union-2.1-8steps.py)|
|[PAI/Z-Image-Turbo-Fun-Controlnet-Tile-2.1-8steps](https://www.modelscope.cn/models/PAI/Z-Image-Turbo-Fun-Controlnet-Union-2.1)|[code](/examples/z_image/model_inference/Z-Image-Turbo-Fun-Controlnet-Tile-2.1-8steps.py)|[code](/examples/z_image/model_inference_low_vram/Z-Image-Turbo-Fun-Controlnet-Tile-2.1-8steps.py)|[code](/examples/z_image/model_training/full/Z-Image-Turbo-Fun-Controlnet-Tile-2.1-8steps.sh)|[code](/examples/z_image/model_training/validate_full/Z-Image-Turbo-Fun-Controlnet-Tile-2.1-8steps.py)|[code](/examples/z_image/model_training/lora/Z-Image-Turbo-Fun-Controlnet-Tile-2.1-8steps.sh)|[code](/examples/z_image/model_training/validate_lora/Z-Image-Turbo-Fun-Controlnet-Tile-2.1-8steps.py)|

</details>

#### FLUX.2: [/docs/zh/Model_Details/FLUX2.md](/docs/zh/Model_Details/FLUX2.md)

<details>

<summary>å¿«é€Ÿå¼€å§‹</summary>

è¿è¡Œä»¥ä¸‹ä»£ç å¯ä»¥å¿«é€ŸåŠ è½½ [black-forest-labs/FLUX.2-dev](https://www.modelscope.cn/models/black-forest-labs/FLUX.2-dev) æ¨¡å‹å¹¶è¿›è¡Œæ¨ç†ã€‚æ˜¾å­˜ç®¡ç†å·²å¯åŠ¨ï¼Œæ¡†æ¶ä¼šè‡ªåŠ¨æ ¹æ®å‰©ä½™æ˜¾å­˜æ§åˆ¶æ¨¡å‹å‚æ•°çš„åŠ è½½ï¼Œæœ€ä½ 10G æ˜¾å­˜å³å¯è¿è¡Œã€‚

```python
from diffsynth.pipelines.flux2_image import Flux2ImagePipeline, ModelConfig
import torch

vram_config = {
    "offload_dtype": "disk",
    "offload_device": "disk",
    "onload_dtype": torch.float8_e4m3fn,
    "onload_device": "cpu",
    "preparing_dtype": torch.float8_e4m3fn,
    "preparing_device": "cuda",
    "computation_dtype": torch.bfloat16,
    "computation_device": "cuda",
}
pipe = Flux2ImagePipeline.from_pretrained(
    torch_dtype=torch.bfloat16,
    device="cuda",
    model_configs=[
        ModelConfig(model_id="black-forest-labs/FLUX.2-dev", origin_file_pattern="text_encoder/*.safetensors", **vram_config),
        ModelConfig(model_id="black-forest-labs/FLUX.2-dev", origin_file_pattern="transformer/*.safetensors", **vram_config),
        ModelConfig(model_id="black-forest-labs/FLUX.2-dev", origin_file_pattern="vae/diffusion_pytorch_model.safetensors"),
    ],
    tokenizer_config=ModelConfig(model_id="black-forest-labs/FLUX.2-dev", origin_file_pattern="tokenizer/"),
    vram_limit=torch.cuda.mem_get_info("cuda")[1] / (1024 ** 3) - 0.5,
)
prompt = "High resolution. A dreamy underwater portrait of a serene young woman in a flowing blue dress. Her hair floats softly around her face, strands delicately suspended in the water. Clear, shimmering light filters through, casting gentle highlights, while tiny bubbles rise around her. Her expression is calm, her features finely detailedâ€”creating a tranquil, ethereal scene."
image = pipe(prompt, seed=42, rand_device="cuda", num_inference_steps=50)
image.save("image.jpg")
```

</details>

<details>

<summary>ç¤ºä¾‹ä»£ç </summary>

FLUX.2 çš„ç¤ºä¾‹ä»£ç ä½äºï¼š[/examples/flux2/](/examples/flux2/)

|æ¨¡å‹ ID|æ¨ç†|ä½æ˜¾å­˜æ¨ç†|å…¨é‡è®­ç»ƒ|å…¨é‡è®­ç»ƒåéªŒè¯|LoRA è®­ç»ƒ|LoRA è®­ç»ƒåéªŒè¯|
|-|-|-|-|-|-|-|
|[black-forest-labs/FLUX.2-dev](https://www.modelscope.cn/models/black-forest-labs/FLUX.2-dev)|[code](/examples/flux2/model_inference/FLUX.2-dev.py)|[code](/examples/flux2/model_inference_low_vram/FLUX.2-dev.py)|-|-|[code](/examples/flux2/model_training/lora/FLUX.2-dev.sh)|[code](/examples/flux2/model_training/validate_lora/FLUX.2-dev.py)|
|[black-forest-labs/FLUX.2-klein-4B](https://www.modelscope.cn/models/black-forest-labs/FLUX.2-klein-4B)|[code](/examples/flux2/model_inference/FLUX.2-klein-4B.py)|[code](/examples/flux2/model_inference_low_vram/FLUX.2-klein-4B.py)|[code](/examples/flux2/model_training/full/FLUX.2-klein-4B.sh)|[code](/examples/flux2/model_training/validate_full/FLUX.2-klein-4B.py)|[code](/examples/flux2/model_training/lora/FLUX.2-klein-4B.sh)|[code](/examples/flux2/model_training/validate_lora/FLUX.2-klein-4B.py)|
|[black-forest-labs/FLUX.2-klein-9B](https://www.modelscope.cn/models/black-forest-labs/FLUX.2-klein-9B)|[code](/examples/flux2/model_inference/FLUX.2-klein-9B.py)|[code](/examples/flux2/model_inference_low_vram/FLUX.2-klein-9B.py)|[code](/examples/flux2/model_training/full/FLUX.2-klein-9B.sh)|[code](/examples/flux2/model_training/validate_full/FLUX.2-klein-9B.py)|[code](/examples/flux2/model_training/lora/FLUX.2-klein-9B.sh)|[code](/examples/flux2/model_training/validate_lora/FLUX.2-klein-9B.py)|
|[black-forest-labs/FLUX.2-klein-base-4B](https://www.modelscope.cn/models/black-forest-labs/FLUX.2-klein-base-4B)|[code](/examples/flux2/model_inference/FLUX.2-klein-base-4B.py)|[code](/examples/flux2/model_inference_low_vram/FLUX.2-klein-base-4B.py)|[code](/examples/flux2/model_training/full/FLUX.2-klein-base-4B.sh)|[code](/examples/flux2/model_training/validate_full/FLUX.2-klein-base-4B.py)|[code](/examples/flux2/model_training/lora/FLUX.2-klein-base-4B.sh)|[code](/examples/flux2/model_training/validate_lora/FLUX.2-klein-base-4B.py)|
|[black-forest-labs/FLUX.2-klein-base-9B](https://www.modelscope.cn/models/black-forest-labs/FLUX.2-klein-base-9B)|[code](/examples/flux2/model_inference/FLUX.2-klein-base-9B.py)|[code](/examples/flux2/model_inference_low_vram/FLUX.2-klein-base-9B.py)|[code](/examples/flux2/model_training/full/FLUX.2-klein-base-9B.sh)|[code](/examples/flux2/model_training/validate_full/FLUX.2-klein-base-9B.py)|[code](/examples/flux2/model_training/lora/FLUX.2-klein-base-9B.sh)|[code](/examples/flux2/model_training/validate_lora/FLUX.2-klein-base-9B.py)|

</details>

#### Qwen-Image: [/docs/zh/Model_Details/Qwen-Image.md](/docs/zh/Model_Details/Qwen-Image.md)

<details>

<summary>å¿«é€Ÿå¼€å§‹</summary>

è¿è¡Œä»¥ä¸‹ä»£ç å¯ä»¥å¿«é€ŸåŠ è½½ [Qwen/Qwen-Image](https://www.modelscope.cn/models/Qwen/Qwen-Image) æ¨¡å‹å¹¶è¿›è¡Œæ¨ç†ã€‚æ˜¾å­˜ç®¡ç†å·²å¯åŠ¨ï¼Œæ¡†æ¶ä¼šè‡ªåŠ¨æ ¹æ®å‰©ä½™æ˜¾å­˜æ§åˆ¶æ¨¡å‹å‚æ•°çš„åŠ è½½ï¼Œæœ€ä½ 8G æ˜¾å­˜å³å¯è¿è¡Œã€‚

```python
from diffsynth.pipelines.qwen_image import QwenImagePipeline, ModelConfig
import torch

vram_config = {
    "offload_dtype": "disk",
    "offload_device": "disk",
    "onload_dtype": torch.float8_e4m3fn,
    "onload_device": "cpu",
    "preparing_dtype": torch.float8_e4m3fn,
    "preparing_device": "cuda",
    "computation_dtype": torch.bfloat16,
    "computation_device": "cuda",
}
pipe = QwenImagePipeline.from_pretrained(
    torch_dtype=torch.bfloat16,
    device="cuda",
    model_configs=[
        ModelConfig(model_id="Qwen/Qwen-Image", origin_file_pattern="transformer/diffusion_pytorch_model*.safetensors", **vram_config),
        ModelConfig(model_id="Qwen/Qwen-Image", origin_file_pattern="text_encoder/model*.safetensors", **vram_config),
        ModelConfig(model_id="Qwen/Qwen-Image", origin_file_pattern="vae/diffusion_pytorch_model.safetensors", **vram_config),
    ],
    tokenizer_config=ModelConfig(model_id="Qwen/Qwen-Image", origin_file_pattern="tokenizer/"),
    vram_limit=torch.cuda.mem_get_info("cuda")[1] / (1024 ** 3) - 0.5,
)
prompt = "ç²¾è‡´è‚–åƒï¼Œæ°´ä¸‹å°‘å¥³ï¼Œè“è£™é£˜é€¸ï¼Œå‘ä¸è½»æ‰¬ï¼Œå…‰å½±é€æ¾ˆï¼Œæ°”æ³¡ç¯ç»•ï¼Œé¢å®¹æ¬é™ï¼Œç»†èŠ‚ç²¾è‡´ï¼Œæ¢¦å¹»å”¯ç¾ã€‚"
image = pipe(prompt, seed=0, num_inference_steps=40)
image.save("image.jpg")
```

</details>

<details>

<summary>æ¨¡å‹è¡€ç¼˜</summary>

```mermaid
graph LR;
    Qwen/Qwen-Image-->Qwen/Qwen-Image-Edit;
    Qwen/Qwen-Image-Edit-->Qwen/Qwen-Image-Edit-2509;
    Qwen/Qwen-Image-->EliGen-Series;
    EliGen-Series-->DiffSynth-Studio/Qwen-Image-EliGen;
    DiffSynth-Studio/Qwen-Image-EliGen-->DiffSynth-Studio/Qwen-Image-EliGen-V2;
    EliGen-Series-->DiffSynth-Studio/Qwen-Image-EliGen-Poster;
    Qwen/Qwen-Image-->Distill-Series;
    Distill-Series-->DiffSynth-Studio/Qwen-Image-Distill-Full;
    Distill-Series-->DiffSynth-Studio/Qwen-Image-Distill-LoRA;
    Qwen/Qwen-Image-->ControlNet-Series;
    ControlNet-Series-->Blockwise-ControlNet-Series;
    Blockwise-ControlNet-Series-->DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Canny;
    Blockwise-ControlNet-Series-->DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Depth;
    Blockwise-ControlNet-Series-->DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Inpaint;
    ControlNet-Series-->DiffSynth-Studio/Qwen-Image-In-Context-Control-Union;
    Qwen/Qwen-Image-->DiffSynth-Studio/Qwen-Image-Edit-Lowres-Fix;
```

</details>

<details>

<summary>ç¤ºä¾‹ä»£ç </summary>

Qwen-Image çš„ç¤ºä¾‹ä»£ç ä½äºï¼š[/examples/qwen_image/](/examples/qwen_image/)

|æ¨¡å‹ ID|æ¨ç†|ä½æ˜¾å­˜æ¨ç†|å…¨é‡è®­ç»ƒ|å…¨é‡è®­ç»ƒåéªŒè¯|LoRA è®­ç»ƒ|LoRA è®­ç»ƒåéªŒè¯|
|-|-|-|-|-|-|-|
|[Qwen/Qwen-Image](https://www.modelscope.cn/models/Qwen/Qwen-Image)|[code](/examples/qwen_image/model_inference/Qwen-Image.py)|[code](/examples/qwen_image/model_inference_low_vram/Qwen-Image.py)|[code](/examples/qwen_image/model_training/full/Qwen-Image.sh)|[code](/examples/qwen_image/model_training/validate_full/Qwen-Image.py)|[code](/examples/qwen_image/model_training/lora/Qwen-Image.sh)|[code](/examples/qwen_image/model_training/validate_lora/Qwen-Image.py)|
|[Qwen/Qwen-Image-2512](https://www.modelscope.cn/models/Qwen/Qwen-Image-2512)|[code](/examples/qwen_image/model_inference/Qwen-Image-2512.py)|[code](/examples/qwen_image/model_inference_low_vram/Qwen-Image-2512.py)|[code](/examples/qwen_image/model_training/full/Qwen-Image-2512.sh)|[code](/examples/qwen_image/model_training/validate_full/Qwen-Image-2512.py)|[code](/examples/qwen_image/model_training/lora/Qwen-Image-2512.sh)|[code](/examples/qwen_image/model_training/validate_lora/Qwen-Image-2512.py)|
|[Qwen/Qwen-Image-Edit](https://www.modelscope.cn/models/Qwen/Qwen-Image-Edit)|[code](/examples/qwen_image/model_inference/Qwen-Image-Edit.py)|[code](/examples/qwen_image/model_inference_low_vram/Qwen-Image-Edit.py)|[code](/examples/qwen_image/model_training/full/Qwen-Image-Edit.sh)|[code](/examples/qwen_image/model_training/validate_full/Qwen-Image-Edit.py)|[code](/examples/qwen_image/model_training/lora/Qwen-Image-Edit.sh)|[code](/examples/qwen_image/model_training/validate_lora/Qwen-Image-Edit.py)|
|[Qwen/Qwen-Image-Edit-2509](https://www.modelscope.cn/models/Qwen/Qwen-Image-Edit-2509)|[code](/examples/qwen_image/model_inference/Qwen-Image-Edit-2509.py)|[code](/examples/qwen_image/model_inference_low_vram/Qwen-Image-Edit-2509.py)|[code](/examples/qwen_image/model_training/full/Qwen-Image-Edit-2509.sh)|[code](/examples/qwen_image/model_training/validate_full/Qwen-Image-Edit-2509.py)|[code](/examples/qwen_image/model_training/lora/Qwen-Image-Edit-2509.sh)|[code](/examples/qwen_image/model_training/validate_lora/Qwen-Image-Edit-2509.py)|
|[Qwen/Qwen-Image-Edit-2511](https://www.modelscope.cn/models/Qwen/Qwen-Image-Edit-2511)|[code](/examples/qwen_image/model_inference/Qwen-Image-Edit-2511.py)|[code](/examples/qwen_image/model_inference_low_vram/Qwen-Image-Edit-2511.py)|[code](/examples/qwen_image/model_training/full/Qwen-Image-Edit-2511.sh)|[code](/examples/qwen_image/model_training/validate_full/Qwen-Image-Edit-2511.py)|[code](/examples/qwen_image/model_training/lora/Qwen-Image-Edit-2511.sh)|[code](/examples/qwen_image/model_training/validate_lora/Qwen-Image-Edit-2511.py)|
|[lightx2v/Qwen-Image-Edit-2511-Lightning](https://modelscope.cn/models/lightx2v/Qwen-Image-Edit-2511-Lightning)|[code](/examples/qwen_image/model_inference/Qwen-Image-Edit-2511-Lightning.py)|[code](/examples/qwen_image/model_inference_low_vram/Qwen-Image-Edit-2511-Lightning.py)|-|-|-|-|
|[Qwen/Qwen-Image-Layered](https://www.modelscope.cn/models/Qwen/Qwen-Image-Layered)|[code](/examples/qwen_image/model_inference/Qwen-Image-Layered.py)|[code](/examples/qwen_image/model_inference_low_vram/Qwen-Image-Layered.py)|[code](/examples/qwen_image/model_training/full/Qwen-Image-Layered.sh)|[code](/examples/qwen_image/model_training/validate_full/Qwen-Image-Layered.py)|[code](/examples/qwen_image/model_training/lora/Qwen-Image-Layered.sh)|[code](/examples/qwen_image/model_training/validate_lora/Qwen-Image-Layered.py)|
|[DiffSynth-Studio/Qwen-Image-Layered-Control](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Layered-Control)|[code](/examples/qwen_image/model_inference/Qwen-Image-Layered-Control.py)|[code](/examples/qwen_image/model_inference_low_vram/Qwen-Image-Layered-Control.py)|[code](/examples/qwen_image/model_training/full/Qwen-Image-Layered-Control.sh)|[code](/examples/qwen_image/model_training/validate_full/Qwen-Image-Layered-Control.py)|[code](/examples/qwen_image/model_training/lora/Qwen-Image-Layered-Control.sh)|[code](/examples/qwen_image/model_training/validate_lora/Qwen-Image-Layered-Control.py)|
|[DiffSynth-Studio/Qwen-Image-EliGen](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-EliGen)|[code](/examples/qwen_image/model_inference/Qwen-Image-EliGen.py)|[code](/examples/qwen_image/model_inference_low_vram/Qwen-Image-EliGen.py)|-|-|[code](/examples/qwen_image/model_training/lora/Qwen-Image-EliGen.sh)|[code](/examples/qwen_image/model_training/validate_lora/Qwen-Image-EliGen.py)|
|[DiffSynth-Studio/Qwen-Image-EliGen-V2](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-EliGen-V2)|[code](/examples/qwen_image/model_inference/Qwen-Image-EliGen-V2.py)|[code](/examples/qwen_image/model_inference_low_vram/Qwen-Image-EliGen-V2.py)|-|-|[code](/examples/qwen_image/model_training/lora/Qwen-Image-EliGen.sh)|[code](/examples/qwen_image/model_training/validate_lora/Qwen-Image-EliGen.py)|
|[DiffSynth-Studio/Qwen-Image-EliGen-Poster](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-EliGen-Poster)|[code](/examples/qwen_image/model_inference/Qwen-Image-EliGen-Poster.py)|[code](/examples/qwen_image/model_inference_low_vram/Qwen-Image-EliGen-Poster.py)|-|-|[code](/examples/qwen_image/model_training/lora/Qwen-Image-EliGen-Poster.sh)|[code](/examples/qwen_image/model_training/validate_lora/Qwen-Image-EliGen-Poster.py)|
|[DiffSynth-Studio/Qwen-Image-Distill-Full](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Distill-Full)|[code](/examples/qwen_image/model_inference/Qwen-Image-Distill-Full.py)|[code](/examples/qwen_image/model_inference_low_vram/Qwen-Image-Distill-Full.py)|[code](/examples/qwen_image/model_training/full/Qwen-Image-Distill-Full.sh)|[code](/examples/qwen_image/model_training/validate_full/Qwen-Image-Distill-Full.py)|[code](/examples/qwen_image/model_training/lora/Qwen-Image-Distill-Full.sh)|[code](/examples/qwen_image/model_training/validate_lora/Qwen-Image-Distill-Full.py)|
|[DiffSynth-Studio/Qwen-Image-Distill-LoRA](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Distill-LoRA)|[code](/examples/qwen_image/model_inference/Qwen-Image-Distill-LoRA.py)|[code](/examples/qwen_image/model_inference_low_vram/Qwen-Image-Distill-LoRA.py)|-|-|[code](/examples/qwen_image/model_training/lora/Qwen-Image-Distill-LoRA.sh)|[code](/examples/qwen_image/model_training/validate_lora/Qwen-Image-Distill-LoRA.py)|
|[DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Canny](https://modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Canny)|[code](/examples/qwen_image/model_inference/Qwen-Image-Blockwise-ControlNet-Canny.py)|[code](/examples/qwen_image/model_inference_low_vram/Qwen-Image-Blockwise-ControlNet-Canny.py)|[code](/examples/qwen_image/model_training/full/Qwen-Image-Blockwise-ControlNet-Canny.sh)|[code](/examples/qwen_image/model_training/validate_full/Qwen-Image-Blockwise-ControlNet-Canny.py)|[code](/examples/qwen_image/model_training/lora/Qwen-Image-Blockwise-ControlNet-Canny.sh)|[code](/examples/qwen_image/model_training/validate_lora/Qwen-Image-Blockwise-ControlNet-Canny.py)|
|[DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Depth](https://modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Depth)|[code](/examples/qwen_image/model_inference/Qwen-Image-Blockwise-ControlNet-Depth.py)|[code](/examples/qwen_image/model_inference_low_vram/Qwen-Image-Blockwise-ControlNet-Depth.py)|[code](/examples/qwen_image/model_training/full/Qwen-Image-Blockwise-ControlNet-Depth.sh)|[code](/examples/qwen_image/model_training/validate_full/Qwen-Image-Blockwise-ControlNet-Depth.py)|[code](/examples/qwen_image/model_training/lora/Qwen-Image-Blockwise-ControlNet-Depth.sh)|[code](/examples/qwen_image/model_training/validate_lora/Qwen-Image-Blockwise-ControlNet-Depth.py)|
|[DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Inpaint](https://modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Inpaint)|[code](/examples/qwen_image/model_inference/Qwen-Image-Blockwise-ControlNet-Inpaint.py)|[code](/examples/qwen_image/model_inference_low_vram/Qwen-Image-Blockwise-ControlNet-Inpaint.py)|[code](/examples/qwen_image/model_training/full/Qwen-Image-Blockwise-ControlNet-Inpaint.sh)|[code](/examples/qwen_image/model_training/validate_full/Qwen-Image-Blockwise-ControlNet-Inpaint.py)|[code](/examples/qwen_image/model_training/lora/Qwen-Image-Blockwise-ControlNet-Inpaint.sh)|[code](/examples/qwen_image/model_training/validate_lora/Qwen-Image-Blockwise-ControlNet-Inpaint.py)|
|[DiffSynth-Studio/Qwen-Image-In-Context-Control-Union](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-In-Context-Control-Union)|[code](/examples/qwen_image/model_inference/Qwen-Image-In-Context-Control-Union.py)|[code](/examples/qwen_image/model_inference_low_vram/Qwen-Image-In-Context-Control-Union.py)|-|-|[code](/examples/qwen_image/model_training/lora/Qwen-Image-In-Context-Control-Union.sh)|[code](/examples/qwen_image/model_training/validate_lora/Qwen-Image-In-Context-Control-Union.py)|
|[DiffSynth-Studio/Qwen-Image-Edit-Lowres-Fix](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Edit-Lowres-Fix)|[code](/examples/qwen_image/model_inference/Qwen-Image-Edit-Lowres-Fix.py)|[code](/examples/qwen_image/model_inference_low_vram/Qwen-Image-Edit-Lowres-Fix.py)|-|-|-|-|
|[DiffSynth-Studio/Qwen-Image-i2L](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-i2L)|[code](/examples/qwen_image/model_inference/Qwen-Image-i2L.py)|[code](/examples/qwen_image/model_inference_low_vram/Qwen-Image-i2L.py)|-|-|-|-|

</details>

#### FLUX.1: [/docs/zh/Model_Details/FLUX.md](/docs/zh/Model_Details/FLUX.md)

<details>

<summary>å¿«é€Ÿå¼€å§‹</summary>

è¿è¡Œä»¥ä¸‹ä»£ç å¯ä»¥å¿«é€ŸåŠ è½½ [black-forest-labs/FLUX.1-dev](https://www.modelscope.cn/models/black-forest-labs/FLUX.1-dev) æ¨¡å‹å¹¶è¿›è¡Œæ¨ç†ã€‚æ˜¾å­˜ç®¡ç†å·²å¯åŠ¨ï¼Œæ¡†æ¶ä¼šè‡ªåŠ¨æ ¹æ®å‰©ä½™æ˜¾å­˜æ§åˆ¶æ¨¡å‹å‚æ•°çš„åŠ è½½ï¼Œæœ€ä½ 8G æ˜¾å­˜å³å¯è¿è¡Œã€‚

```python
import torch
from diffsynth.pipelines.flux_image import FluxImagePipeline, ModelConfig

vram_config = {
    "offload_dtype": torch.float8_e4m3fn,
    "offload_device": "cpu",
    "onload_dtype": torch.float8_e4m3fn,
    "onload_device": "cpu",
    "preparing_dtype": torch.float8_e4m3fn,
    "preparing_device": "cuda",
    "computation_dtype": torch.bfloat16,
    "computation_device": "cuda",
}
pipe = FluxImagePipeline.from_pretrained(
    torch_dtype=torch.bfloat16,
    device="cuda",
    model_configs=[
        ModelConfig(model_id="black-forest-labs/FLUX.1-dev", origin_file_pattern="flux1-dev.safetensors", **vram_config),
        ModelConfig(model_id="black-forest-labs/FLUX.1-dev", origin_file_pattern="text_encoder/model.safetensors", **vram_config),
        ModelConfig(model_id="black-forest-labs/FLUX.1-dev", origin_file_pattern="text_encoder_2/*.safetensors", **vram_config),
        ModelConfig(model_id="black-forest-labs/FLUX.1-dev", origin_file_pattern="ae.safetensors", **vram_config),
    ],
    vram_limit=torch.cuda.mem_get_info("cuda")[1] / (1024 ** 3) - 1,
)
prompt = "CG, masterpiece, best quality, solo, long hair, wavy hair, silver hair, blue eyes, blue dress, medium breasts, dress, underwater, air bubble, floating hair, refraction, portrait. The girl's flowing silver hair shimmers with every color of the rainbow and cascades down, merging with the floating flora around her."
image = pipe(prompt=prompt, seed=0)
image.save("image.jpg")
```

</details>

<details>

<summary>æ¨¡å‹è¡€ç¼˜</summary>

```mermaid
graph LR;
    FLUX.1-Series-->black-forest-labs/FLUX.1-dev;
    FLUX.1-Series-->black-forest-labs/FLUX.1-Krea-dev;
    FLUX.1-Series-->black-forest-labs/FLUX.1-Kontext-dev;
    black-forest-labs/FLUX.1-dev-->FLUX.1-dev-ControlNet-Series;
    FLUX.1-dev-ControlNet-Series-->alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Beta;
    FLUX.1-dev-ControlNet-Series-->InstantX/FLUX.1-dev-Controlnet-Union-alpha;
    FLUX.1-dev-ControlNet-Series-->jasperai/Flux.1-dev-Controlnet-Upscaler;
    black-forest-labs/FLUX.1-dev-->InstantX/FLUX.1-dev-IP-Adapter;
    black-forest-labs/FLUX.1-dev-->ByteDance/InfiniteYou;
    black-forest-labs/FLUX.1-dev-->DiffSynth-Studio/Eligen;
    black-forest-labs/FLUX.1-dev-->DiffSynth-Studio/LoRA-Encoder-FLUX.1-Dev;
    black-forest-labs/FLUX.1-dev-->DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev;
    black-forest-labs/FLUX.1-dev-->ostris/Flex.2-preview;
    black-forest-labs/FLUX.1-dev-->stepfun-ai/Step1X-Edit;
    Qwen/Qwen2.5-VL-7B-Instruct-->stepfun-ai/Step1X-Edit;
    black-forest-labs/FLUX.1-dev-->DiffSynth-Studio/Nexus-GenV2;
    Qwen/Qwen2.5-VL-7B-Instruct-->DiffSynth-Studio/Nexus-GenV2;
```

</details>

<details>

<summary>ç¤ºä¾‹ä»£ç </summary>

FLUX.1 çš„ç¤ºä¾‹ä»£ç ä½äºï¼š[/examples/flux/](/examples/flux/)

|æ¨¡å‹ ID|é¢å¤–å‚æ•°|æ¨ç†|ä½æ˜¾å­˜æ¨ç†|å…¨é‡è®­ç»ƒ|å…¨é‡è®­ç»ƒåéªŒè¯|LoRA è®­ç»ƒ|LoRA è®­ç»ƒåéªŒè¯|
|-|-|-|-|-|-|-|-|
|[black-forest-labs/FLUX.1-dev](https://www.modelscope.cn/models/black-forest-labs/FLUX.1-dev)||[code](/examples/flux/model_inference/FLUX.1-dev.py)|[code](/examples/flux/model_inference_low_vram/FLUX.1-dev.py)|[code](/examples/flux/model_training/full/FLUX.1-dev.sh)|[code](/examples/flux/model_training/validate_full/FLUX.1-dev.py)|[code](/examples/flux/model_training/lora/FLUX.1-dev.sh)|[code](/examples/flux/model_training/validate_lora/FLUX.1-dev.py)|
|[black-forest-labs/FLUX.1-Krea-dev](https://www.modelscope.cn/models/black-forest-labs/FLUX.1-Krea-dev)||[code](/examples/flux/model_inference/FLUX.1-Krea-dev.py)|[code](/examples/flux/model_inference_low_vram/FLUX.1-Krea-dev.py)|[code](/examples/flux/model_training/full/FLUX.1-Krea-dev.sh)|[code](/examples/flux/model_training/validate_full/FLUX.1-Krea-dev.py)|[code](/examples/flux/model_training/lora/FLUX.1-Krea-dev.sh)|[code](/examples/flux/model_training/validate_lora/FLUX.1-Krea-dev.py)|
|[black-forest-labs/FLUX.1-Kontext-dev](https://www.modelscope.cn/models/black-forest-labs/FLUX.1-Kontext-dev)|`kontext_images`|[code](/examples/flux/model_inference/FLUX.1-Kontext-dev.py)|[code](/examples/flux/model_inference_low_vram/FLUX.1-Kontext-dev.py)|[code](/examples/flux/model_training/full/FLUX.1-Kontext-dev.sh)|[code](/examples/flux/model_training/validate_full/FLUX.1-Kontext-dev.py)|[code](/examples/flux/model_training/lora/FLUX.1-Kontext-dev.sh)|[code](/examples/flux/model_training/validate_lora/FLUX.1-Kontext-dev.py)|
|[alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Beta](https://www.modelscope.cn/models/alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Beta)|`controlnet_inputs`|[code](/examples/flux/model_inference/FLUX.1-dev-Controlnet-Inpainting-Beta.py)|[code](/examples/flux/model_inference_low_vram/FLUX.1-dev-Controlnet-Inpainting-Beta.py)|[code](/examples/flux/model_training/full/FLUX.1-dev-Controlnet-Inpainting-Beta.sh)|[code](/examples/flux/model_training/validate_full/FLUX.1-dev-Controlnet-Inpainting-Beta.py)|[code](/examples/flux/model_training/lora/FLUX.1-dev-Controlnet-Inpainting-Beta.sh)|[code](/examples/flux/model_training/validate_lora/FLUX.1-dev-Controlnet-Inpainting-Beta.py)|
|[InstantX/FLUX.1-dev-Controlnet-Union-alpha](https://www.modelscope.cn/models/InstantX/FLUX.1-dev-Controlnet-Union-alpha)|`controlnet_inputs`|[code](/examples/flux/model_inference/FLUX.1-dev-Controlnet-Union-alpha.py)|[code](/examples/flux/model_inference_low_vram/FLUX.1-dev-Controlnet-Union-alpha.py)|[code](/examples/flux/model_training/full/FLUX.1-dev-Controlnet-Union-alpha.sh)|[code](/examples/flux/model_training/validate_full/FLUX.1-dev-Controlnet-Union-alpha.py)|[code](/examples/flux/model_training/lora/FLUX.1-dev-Controlnet-Union-alpha.sh)|[code](/examples/flux/model_training/validate_lora/FLUX.1-dev-Controlnet-Union-alpha.py)|
|[jasperai/Flux.1-dev-Controlnet-Upscaler](https://www.modelscope.cn/models/jasperai/Flux.1-dev-Controlnet-Upscaler)|`controlnet_inputs`|[code](/examples/flux/model_inference/FLUX.1-dev-Controlnet-Upscaler.py)|[code](/examples/flux/model_inference_low_vram/FLUX.1-dev-Controlnet-Upscaler.py)|[code](/examples/flux/model_training/full/FLUX.1-dev-Controlnet-Upscaler.sh)|[code](/examples/flux/model_training/validate_full/FLUX.1-dev-Controlnet-Upscaler.py)|[code](/examples/flux/model_training/lora/FLUX.1-dev-Controlnet-Upscaler.sh)|[code](/examples/flux/model_training/validate_lora/FLUX.1-dev-Controlnet-Upscaler.py)|
|[InstantX/FLUX.1-dev-IP-Adapter](https://www.modelscope.cn/models/InstantX/FLUX.1-dev-IP-Adapter)|`ipadapter_images`, `ipadapter_scale`|[code](/examples/flux/model_inference/FLUX.1-dev-IP-Adapter.py)|[code](/examples/flux/model_inference_low_vram/FLUX.1-dev-IP-Adapter.py)|[code](/examples/flux/model_training/full/FLUX.1-dev-IP-Adapter.sh)|[code](/examples/flux/model_training/validate_full/FLUX.1-dev-IP-Adapter.py)|[code](/examples/flux/model_training/lora/FLUX.1-dev-IP-Adapter.sh)|[code](/examples/flux/model_training/validate_lora/FLUX.1-dev-IP-Adapter.py)|
|[ByteDance/InfiniteYou](https://www.modelscope.cn/models/ByteDance/InfiniteYou)|`infinityou_id_image`, `infinityou_guidance`, `controlnet_inputs`|[code](/examples/flux/model_inference/FLUX.1-dev-InfiniteYou.py)|[code](/examples/flux/model_inference_low_vram/FLUX.1-dev-InfiniteYou.py)|[code](/examples/flux/model_training/full/FLUX.1-dev-InfiniteYou.sh)|[code](/examples/flux/model_training/validate_full/FLUX.1-dev-InfiniteYou.py)|[code](/examples/flux/model_training/lora/FLUX.1-dev-InfiniteYou.sh)|[code](/examples/flux/model_training/validate_lora/FLUX.1-dev-InfiniteYou.py)|
|[DiffSynth-Studio/Eligen](https://www.modelscope.cn/models/DiffSynth-Studio/Eligen)|`eligen_entity_prompts`, `eligen_entity_masks`, `eligen_enable_on_negative`, `eligen_enable_inpaint`|[code](/examples/flux/model_inference/FLUX.1-dev-EliGen.py)|[code](/examples/flux/model_inference_low_vram/FLUX.1-dev-EliGen.py)|-|-|[code](/examples/flux/model_training/lora/FLUX.1-dev-EliGen.sh)|[code](/examples/flux/model_training/validate_lora/FLUX.1-dev-EliGen.py)|
|[DiffSynth-Studio/LoRA-Encoder-FLUX.1-Dev](https://www.modelscope.cn/models/DiffSynth-Studio/LoRA-Encoder-FLUX.1-Dev)|`lora_encoder_inputs`, `lora_encoder_scale`|[code](/examples/flux/model_inference/FLUX.1-dev-LoRA-Encoder.py)|[code](/examples/flux/model_inference_low_vram/FLUX.1-dev-LoRA-Encoder.py)|[code](/examples/flux/model_training/full/FLUX.1-dev-LoRA-Encoder.sh)|[code](/examples/flux/model_training/validate_full/FLUX.1-dev-LoRA-Encoder.py)|-|-|
|[DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev](https://modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev)||[code](/examples/flux/model_inference/FLUX.1-dev-LoRA-Fusion.py)|-|-|-|-|-|
|[stepfun-ai/Step1X-Edit](https://www.modelscope.cn/models/stepfun-ai/Step1X-Edit)|`step1x_reference_image`|[code](/examples/flux/model_inference/Step1X-Edit.py)|[code](/examples/flux/model_inference_low_vram/Step1X-Edit.py)|[code](/examples/flux/model_training/full/Step1X-Edit.sh)|[code](/examples/flux/model_training/validate_full/Step1X-Edit.py)|[code](/examples/flux/model_training/lora/Step1X-Edit.sh)|[code](/examples/flux/model_training/validate_lora/Step1X-Edit.py)|
|[ostris/Flex.2-preview](https://www.modelscope.cn/models/ostris/Flex.2-preview)|`flex_inpaint_image`, `flex_inpaint_mask`, `flex_control_image`, `flex_control_strength`, `flex_control_stop`|[code](/examples/flux/model_inference/FLEX.2-preview.py)|[code](/examples/flux/model_inference_low_vram/FLEX.2-preview.py)|[code](/examples/flux/model_training/full/FLEX.2-preview.sh)|[code](/examples/flux/model_training/validate_full/FLEX.2-preview.py)|[code](/examples/flux/model_training/lora/FLEX.2-preview.sh)|[code](/examples/flux/model_training/validate_lora/FLEX.2-preview.py)|
|[DiffSynth-Studio/Nexus-GenV2](https://www.modelscope.cn/models/DiffSynth-Studio/Nexus-GenV2)|`nexus_gen_reference_image`|[code](/examples/flux/model_inference/Nexus-Gen-Editing.py)|[code](/examples/flux/model_inference_low_vram/Nexus-Gen-Editing.py)|[code](/examples/flux/model_training/full/Nexus-Gen.sh)|[code](/examples/flux/model_training/validate_full/Nexus-Gen.py)|[code](/examples/flux/model_training/lora/Nexus-Gen.sh)|[code](/examples/flux/model_training/validate_lora/Nexus-Gen.py)|

</details>

### è§†é¢‘ç”Ÿæˆæ¨¡å‹

https://github.com/user-attachments/assets/1d66ae74-3b02-40a9-acc3-ea95fc039314

#### LTX-2: [/docs/zh/Model_Details/LTX-2.md](/docs/zh/Model_Details/LTX-2.md)

<details>

<summary>å¿«é€Ÿå¼€å§‹</summary>

è¿è¡Œä»¥ä¸‹ä»£ç å¯ä»¥å¿«é€ŸåŠ è½½ [Lightricks/LTX-2](https://www.modelscope.cn/models/Lightricks/LTX-2) æ¨¡å‹å¹¶è¿›è¡Œæ¨ç†ã€‚æ˜¾å­˜ç®¡ç†å·²å¯åŠ¨ï¼Œæ¡†æ¶ä¼šè‡ªåŠ¨æ ¹æ®å‰©ä½™æ˜¾å­˜æ§åˆ¶æ¨¡å‹å‚æ•°çš„åŠ è½½ï¼Œæœ€ä½ 8GB æ˜¾å­˜å³å¯è¿è¡Œã€‚

```python
import torch
from diffsynth.pipelines.ltx2_audio_video import LTX2AudioVideoPipeline, ModelConfig
from diffsynth.utils.data.media_io_ltx2 import write_video_audio_ltx2

vram_config = {
    "offload_dtype": torch.float8_e5m2,
    "offload_device": "cpu",
    "onload_dtype": torch.float8_e5m2,
    "onload_device": "cpu",
    "preparing_dtype": torch.float8_e5m2,
    "preparing_device": "cuda",
    "computation_dtype": torch.bfloat16,
    "computation_device": "cuda",
}
pipe = LTX2AudioVideoPipeline.from_pretrained(
    torch_dtype=torch.bfloat16,
    device="cuda",
    model_configs=[
        ModelConfig(model_id="google/gemma-3-12b-it-qat-q4_0-unquantized", origin_file_pattern="model-*.safetensors", **vram_config),
        ModelConfig(model_id="Lightricks/LTX-2", origin_file_pattern="ltx-2-19b-dev.safetensors", **vram_config),
        ModelConfig(model_id="Lightricks/LTX-2", origin_file_pattern="ltx-2-spatial-upscaler-x2-1.0.safetensors", **vram_config),
    ],
    tokenizer_config=ModelConfig(model_id="google/gemma-3-12b-it-qat-q4_0-unquantized"),
    stage2_lora_config=ModelConfig(model_id="Lightricks/LTX-2", origin_file_pattern="ltx-2-19b-distilled-lora-384.safetensors"),
    vram_limit=torch.cuda.mem_get_info("cuda")[1] / (1024 ** 3) - 0.5,
)

prompt = "A girl is very happy, she is speaking: \"I enjoy working with Diffsynth-Studio, it's a perfect framework.\""
negative_prompt = (
    "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, "
    "grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, "
    "deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, "
    "wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of "
    "field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent "
    "lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny "
    "valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, "
    "mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, "
    "off-sync audio, incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward "
    "pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, "
    "inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."
)
height, width, num_frames = 512 * 2, 768 * 2, 121
video, audio = pipe(
    prompt=prompt,
    negative_prompt=negative_prompt,
    seed=43,
    height=height,
    width=width,
    num_frames=num_frames,
    tiled=True,
    use_two_stage_pipeline=True,
)
write_video_audio_ltx2(
    video=video,
    audio=audio,
    output_path='ltx2_twostage.mp4',
    fps=24,
    audio_sample_rate=24000,
)
```

</details>

<details>

<summary>ç¤ºä¾‹ä»£ç </summary>

LTX-2 çš„ç¤ºä¾‹ä»£ç ä½äºï¼š[/examples/ltx2/](/examples/ltx2/)

|æ¨¡å‹ ID|é¢å¤–å‚æ•°|æ¨ç†|ä½æ˜¾å­˜æ¨ç†|å…¨é‡è®­ç»ƒ|å…¨é‡è®­ç»ƒåéªŒè¯|LoRA è®­ç»ƒ|LoRA è®­ç»ƒåéªŒè¯|
|-|-|-|-|-|-|-|-|
|[Lightricks/LTX-2: OneStagePipeline-T2AV](https://www.modelscope.cn/models/Lightricks/LTX-2)||[code](/examples/ltx2/model_inference/LTX-2-T2AV-OneStage.py)|[code](/examples/ltx2/model_inference_low_vram/LTX-2-T2AV-OneStage.py)|-|-|-|-|
|[Lightricks/LTX-2: TwoStagePipeline-T2AV](https://www.modelscope.cn/models/Lightricks/LTX-2)||[code](/examples/ltx2/model_inference/LTX-2-T2AV-TwoStage.py)|[code](/examples/ltx2/model_inference_low_vram/LTX-2-T2AV-TwoStage.py)|-|-|-|-|
|[Lightricks/LTX-2: DistilledPipeline-T2AV](https://www.modelscope.cn/models/Lightricks/LTX-2)||[code](/examples/ltx2/model_inference/LTX-2-T2AV-DistilledPipeline.py)|[code](/examples/ltx2/model_inference_low_vram/LTX-2-T2AV-DistilledPipeline.py)|-|-|-|-|
|[Lightricks/LTX-2: OneStagePipeline-I2AV](https://www.modelscope.cn/models/Lightricks/LTX-2)|`input_images`|[code](/examples/ltx2/model_inference/LTX-2-I2AV-OneStage.py)|[code](/examples/ltx2/model_inference_low_vram/LTX-2-I2AV-OneStage.py)|-|-|-|-|
|[Lightricks/LTX-2: TwoStagePipeline-I2AV](https://www.modelscope.cn/models/Lightricks/LTX-2)|`input_images`|[code](/examples/ltx2/model_inference/LTX-2-I2AV-TwoStage.py)|[code](/examples/ltx2/model_inference_low_vram/LTX-2-I2AV-TwoStage.py)|-|-|-|-|
|[Lightricks/LTX-2: DistilledPipeline-I2AV](https://www.modelscope.cn/models/Lightricks/LTX-2)|`input_images`|[code](/examples/ltx2/model_inference/LTX-2-I2AV-DistilledPipeline.py)|[code](/examples/ltx2/model_inference_low_vram/LTX-2-I2AV-DistilledPipeline.py)|-|-|-|-|

</details>

#### Wan: [/docs/zh/Model_Details/Wan.md](/docs/zh/Model_Details/Wan.md)

<details>

<summary>å¿«é€Ÿå¼€å§‹</summary>

è¿è¡Œä»¥ä¸‹ä»£ç å¯ä»¥å¿«é€ŸåŠ è½½ [Wan-AI/Wan2.1-T2V-1.3B](https://modelscope.cn/models/Wan-AI/Wan2.1-T2V-1.3B) æ¨¡å‹å¹¶è¿›è¡Œæ¨ç†ã€‚æ˜¾å­˜ç®¡ç†å·²å¯åŠ¨ï¼Œæ¡†æ¶ä¼šè‡ªåŠ¨æ ¹æ®å‰©ä½™æ˜¾å­˜æ§åˆ¶æ¨¡å‹å‚æ•°çš„åŠ è½½ï¼Œæœ€ä½ 8G æ˜¾å­˜å³å¯è¿è¡Œã€‚

```python
import torch
from diffsynth.utils.data import save_video, VideoData
from diffsynth.pipelines.wan_video import WanVideoPipeline, ModelConfig

vram_config = {
    "offload_dtype": "disk",
    "offload_device": "disk",
    "onload_dtype": torch.bfloat16,
    "onload_device": "cpu",
    "preparing_dtype": torch.bfloat16,
    "preparing_device": "cuda",
    "computation_dtype": torch.bfloat16,
    "computation_device": "cuda",
}
pipe = WanVideoPipeline.from_pretrained(
    torch_dtype=torch.bfloat16,
    device="cuda",
    model_configs=[
        ModelConfig(model_id="Wan-AI/Wan2.1-T2V-1.3B", origin_file_pattern="diffusion_pytorch_model*.safetensors", **vram_config),
        ModelConfig(model_id="Wan-AI/Wan2.1-T2V-1.3B", origin_file_pattern="models_t5_umt5-xxl-enc-bf16.pth", **vram_config),
        ModelConfig(model_id="Wan-AI/Wan2.1-T2V-1.3B", origin_file_pattern="Wan2.1_VAE.pth", **vram_config),
    ],
    tokenizer_config=ModelConfig(model_id="Wan-AI/Wan2.1-T2V-1.3B", origin_file_pattern="google/umt5-xxl/"),
    vram_limit=torch.cuda.mem_get_info("cuda")[1] / (1024 ** 3) - 2,
)

video = pipe(
    prompt="çºªå®æ‘„å½±é£æ ¼ç”»é¢ï¼Œä¸€åªæ´»æ³¼çš„å°ç‹—åœ¨ç»¿èŒµèŒµçš„è‰åœ°ä¸Šè¿…é€Ÿå¥”è·‘ã€‚å°ç‹—æ¯›è‰²æ£•é»„ï¼Œä¸¤åªè€³æœµç«‹èµ·ï¼Œç¥æƒ…ä¸“æ³¨è€Œæ¬¢å¿«ã€‚é˜³å…‰æ´’åœ¨å®ƒèº«ä¸Šï¼Œä½¿å¾—æ¯›å‘çœ‹ä¸Šå»æ ¼å¤–æŸ”è½¯è€Œé—ªäº®ã€‚èƒŒæ™¯æ˜¯ä¸€ç‰‡å¼€é˜”çš„è‰åœ°ï¼Œå¶å°”ç‚¹ç¼€ç€å‡ æœµé‡èŠ±ï¼Œè¿œå¤„éšçº¦å¯è§è“å¤©å’Œå‡ ç‰‡ç™½äº‘ã€‚é€è§†æ„Ÿé²œæ˜ï¼Œæ•æ‰å°ç‹—å¥”è·‘æ—¶çš„åŠ¨æ„Ÿå’Œå››å‘¨è‰åœ°çš„ç”Ÿæœºã€‚ä¸­æ™¯ä¾§é¢ç§»åŠ¨è§†è§’ã€‚",
    negative_prompt="è‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£æ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œæ•´ä½“å‘ç°ï¼Œæœ€å·®è´¨é‡ï¼Œä½è´¨é‡ï¼ŒJPEGå‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œæ‰‹æŒ‡èåˆï¼Œé™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œæ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°",
    seed=0, tiled=True,
)
save_video(video, "video.mp4", fps=15, quality=5)
```

</details>

<details>

<summary>æ¨¡å‹è¡€ç¼˜</summary>

```mermaid
graph LR;
    Wan-Series-->Wan2.1-Series;
    Wan-Series-->Wan2.2-Series;
    Wan2.1-Series-->Wan-AI/Wan2.1-T2V-1.3B;
    Wan2.1-Series-->Wan-AI/Wan2.1-T2V-14B;
    Wan-AI/Wan2.1-T2V-14B-->Wan-AI/Wan2.1-I2V-14B-480P;
    Wan-AI/Wan2.1-I2V-14B-480P-->Wan-AI/Wan2.1-I2V-14B-720P;
    Wan-AI/Wan2.1-T2V-14B-->Wan-AI/Wan2.1-FLF2V-14B-720P;
    Wan-AI/Wan2.1-T2V-1.3B-->iic/VACE-Wan2.1-1.3B-Preview;
    iic/VACE-Wan2.1-1.3B-Preview-->Wan-AI/Wan2.1-VACE-1.3B;
    Wan-AI/Wan2.1-T2V-14B-->Wan-AI/Wan2.1-VACE-14B;
    Wan-AI/Wan2.1-T2V-1.3B-->Wan2.1-Fun-1.3B-Series;
    Wan2.1-Fun-1.3B-Series-->PAI/Wan2.1-Fun-1.3B-InP;
    Wan2.1-Fun-1.3B-Series-->PAI/Wan2.1-Fun-1.3B-Control;
    Wan-AI/Wan2.1-T2V-14B-->Wan2.1-Fun-14B-Series;
    Wan2.1-Fun-14B-Series-->PAI/Wan2.1-Fun-14B-InP;
    Wan2.1-Fun-14B-Series-->PAI/Wan2.1-Fun-14B-Control;
    Wan-AI/Wan2.1-T2V-1.3B-->Wan2.1-Fun-V1.1-1.3B-Series;
    Wan2.1-Fun-V1.1-1.3B-Series-->PAI/Wan2.1-Fun-V1.1-1.3B-Control;
    Wan2.1-Fun-V1.1-1.3B-Series-->PAI/Wan2.1-Fun-V1.1-1.3B-InP;
    Wan2.1-Fun-V1.1-1.3B-Series-->PAI/Wan2.1-Fun-V1.1-1.3B-Control-Camera;
    Wan-AI/Wan2.1-T2V-14B-->Wan2.1-Fun-V1.1-14B-Series;
    Wan2.1-Fun-V1.1-14B-Series-->PAI/Wan2.1-Fun-V1.1-14B-Control;
    Wan2.1-Fun-V1.1-14B-Series-->PAI/Wan2.1-Fun-V1.1-14B-InP;
    Wan2.1-Fun-V1.1-14B-Series-->PAI/Wan2.1-Fun-V1.1-14B-Control-Camera;
    Wan-AI/Wan2.1-T2V-1.3B-->DiffSynth-Studio/Wan2.1-1.3b-speedcontrol-v1;
    Wan-AI/Wan2.1-T2V-14B-->krea/krea-realtime-video;
    Wan-AI/Wan2.1-T2V-14B-->meituan-longcat/LongCat-Video;
    Wan-AI/Wan2.1-I2V-14B-720P-->ByteDance/Video-As-Prompt-Wan2.1-14B;
    Wan-AI/Wan2.1-T2V-14B-->Wan-AI/Wan2.2-Animate-14B;
    Wan-AI/Wan2.1-T2V-14B-->Wan-AI/Wan2.2-S2V-14B;
    Wan2.2-Series-->Wan-AI/Wan2.2-T2V-A14B;
    Wan2.2-Series-->Wan-AI/Wan2.2-I2V-A14B;
    Wan2.2-Series-->Wan-AI/Wan2.2-TI2V-5B;
    Wan-AI/Wan2.2-T2V-A14B-->Wan2.2-Fun-Series;
    Wan2.2-Fun-Series-->PAI/Wan2.2-VACE-Fun-A14B;
    Wan2.2-Fun-Series-->PAI/Wan2.2-Fun-A14B-InP;
    Wan2.2-Fun-Series-->PAI/Wan2.2-Fun-A14B-Control;
    Wan2.2-Fun-Series-->PAI/Wan2.2-Fun-A14B-Control-Camera;
```

</details>

<details>

<summary>ç¤ºä¾‹ä»£ç </summary>

Wan çš„ç¤ºä¾‹ä»£ç ä½äºï¼š[/examples/wanvideo/](/examples/wanvideo/)

|æ¨¡å‹ ID|é¢å¤–å‚æ•°|æ¨ç†|å…¨é‡è®­ç»ƒ|å…¨é‡è®­ç»ƒåéªŒè¯|LoRA è®­ç»ƒ|LoRA è®­ç»ƒåéªŒè¯|
|-|-|-|-|-|-|-|
|[Wan-AI/Wan2.1-T2V-1.3B](https://modelscope.cn/models/Wan-AI/Wan2.1-T2V-1.3B)||[code](/examples/wanvideo/model_inference/Wan2.1-T2V-1.3B.py)|[code](/examples/wanvideo/model_training/full/Wan2.1-T2V-1.3B.sh)|[code](/examples/wanvideo/model_training/validate_full/Wan2.1-T2V-1.3B.py)|[code](/examples/wanvideo/model_training/lora/Wan2.1-T2V-1.3B.sh)|[code](/examples/wanvideo/model_training/validate_lora/Wan2.1-T2V-1.3B.py)|
|[Wan-AI/Wan2.1-T2V-14B](https://modelscope.cn/models/Wan-AI/Wan2.1-T2V-14B)||[code](/examples/wanvideo/model_inference/Wan2.1-T2V-14B.py)|[code](/examples/wanvideo/model_training/full/Wan2.1-T2V-14B.sh)|[code](/examples/wanvideo/model_training/validate_full/Wan2.1-T2V-14B.py)|[code](/examples/wanvideo/model_training/lora/Wan2.1-T2V-14B.sh)|[code](/examples/wanvideo/model_training/validate_lora/Wan2.1-T2V-14B.py)|
|[Wan-AI/Wan2.1-I2V-14B-480P](https://modelscope.cn/models/Wan-AI/Wan2.1-I2V-14B-480P)|`input_image`|[code](/examples/wanvideo/model_inference/Wan2.1-I2V-14B-480P.py)|[code](/examples/wanvideo/model_training/full/Wan2.1-I2V-14B-480P.sh)|[code](/examples/wanvideo/model_training/validate_full/Wan2.1-I2V-14B-480P.py)|[code](/examples/wanvideo/model_training/lora/Wan2.1-I2V-14B-480P.sh)|[code](/examples/wanvideo/model_training/validate_lora/Wan2.1-I2V-14B-480P.py)|
|[Wan-AI/Wan2.1-I2V-14B-720P](https://modelscope.cn/models/Wan-AI/Wan2.1-I2V-14B-720P)|`input_image`|[code](/examples/wanvideo/model_inference/Wan2.1-I2V-14B-720P.py)|[code](/examples/wanvideo/model_training/full/Wan2.1-I2V-14B-720P.sh)|[code](/examples/wanvideo/model_training/validate_full/Wan2.1-I2V-14B-720P.py)|[code](/examples/wanvideo/model_training/lora/Wan2.1-I2V-14B-720P.sh)|[code](/examples/wanvideo/model_training/validate_lora/Wan2.1-I2V-14B-720P.py)|
|[Wan-AI/Wan2.1-FLF2V-14B-720P](https://modelscope.cn/models/Wan-AI/Wan2.1-FLF2V-14B-720P)|`input_image`, `end_image`|[code](/examples/wanvideo/model_inference/Wan2.1-FLF2V-14B-720P.py)|[code](/examples/wanvideo/model_training/full/Wan2.1-FLF2V-14B-720P.sh)|[code](/examples/wanvideo/model_training/validate_full/Wan2.1-FLF2V-14B-720P.py)|[code](/examples/wanvideo/model_training/lora/Wan2.1-FLF2V-14B-720P.sh)|[code](/examples/wanvideo/model_training/validate_lora/Wan2.1-FLF2V-14B-720P.py)|
|[iic/VACE-Wan2.1-1.3B-Preview](https://modelscope.cn/models/iic/VACE-Wan2.1-1.3B-Preview)|`vace_control_video`, `vace_reference_image`|[code](/examples/wanvideo/model_inference/Wan2.1-VACE-1.3B-Preview.py)|[code](/examples/wanvideo/model_training/full/Wan2.1-VACE-1.3B-Preview.sh)|[code](/examples/wanvideo/model_training/validate_full/Wan2.1-VACE-1.3B-Preview.py)|[code](/examples/wanvideo/model_training/lora/Wan2.1-VACE-1.3B-Preview.sh)|[code](/examples/wanvideo/model_training/validate_lora/Wan2.1-VACE-1.3B-Preview.py)|
|[Wan-AI/Wan2.1-VACE-1.3B](https://modelscope.cn/models/Wan-AI/Wan2.1-VACE-1.3B)|`vace_control_video`, `vace_reference_image`|[code](/examples/wanvideo/model_inference/Wan2.1-VACE-1.3B.py)|[code](/examples/wanvideo/model_training/full/Wan2.1-VACE-1.3B.sh)|[code](/examples/wanvideo/model_training/validate_full/Wan2.1-VACE-1.3B.py)|[code](/examples/wanvideo/model_training/lora/Wan2.1-VACE-1.3B.sh)|[code](/examples/wanvideo/model_training/validate_lora/Wan2.1-VACE-1.3B.py)|
|[Wan-AI/Wan2.1-VACE-14B](https://modelscope.cn/models/Wan-AI/Wan2.1-VACE-14B)|`vace_control_video`, `vace_reference_image`|[code](/examples/wanvideo/model_inference/Wan2.1-VACE-14B.py)|[code](/examples/wanvideo/model_training/full/Wan2.1-VACE-14B.sh)|[code](/examples/wanvideo/model_training/validate_full/Wan2.1-VACE-14B.py)|[code](/examples/wanvideo/model_training/lora/Wan2.1-VACE-14B.sh)|[code](/examples/wanvideo/model_training/validate_lora/Wan2.1-VACE-14B.py)|
|[PAI/Wan2.1-Fun-1.3B-InP](https://modelscope.cn/models/PAI/Wan2.1-Fun-1.3B-InP)|`input_image`, `end_image`|[code](/examples/wanvideo/model_inference/Wan2.1-Fun-1.3B-InP.py)|[code](/examples/wanvideo/model_training/full/Wan2.1-Fun-1.3B-InP.sh)|[code](/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-1.3B-InP.py)|[code](/examples/wanvideo/model_training/lora/Wan2.1-Fun-1.3B-InP.sh)|[code](/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-1.3B-InP.py)|
|[PAI/Wan2.1-Fun-1.3B-Control](https://modelscope.cn/models/PAI/Wan2.1-Fun-1.3B-Control)|`control_video`|[code](/examples/wanvideo/model_inference/Wan2.1-Fun-1.3B-Control.py)|[code](/examples/wanvideo/model_training/full/Wan2.1-Fun-1.3B-Control.sh)|[code](/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-1.3B-Control.py)|[code](/examples/wanvideo/model_training/lora/Wan2.1-Fun-1.3B-Control.sh)|[code](/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-1.3B-Control.py)|
|[PAI/Wan2.1-Fun-14B-InP](https://modelscope.cn/models/PAI/Wan2.1-Fun-14B-InP)|`input_image`, `end_image`|[code](/examples/wanvideo/model_inference/Wan2.1-Fun-14B-InP.py)|[code](/examples/wanvideo/model_training/full/Wan2.1-Fun-14B-InP.sh)|[code](/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-14B-InP.py)|[code](/examples/wanvideo/model_training/lora/Wan2.1-Fun-14B-InP.sh)|[code](/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-14B-InP.py)|
|[PAI/Wan2.1-Fun-14B-Control](https://modelscope.cn/models/PAI/Wan2.1-Fun-14B-Control)|`control_video`|[code](/examples/wanvideo/model_inference/Wan2.1-Fun-14B-Control.py)|[code](/examples/wanvideo/model_training/full/Wan2.1-Fun-14B-Control.sh)|[code](/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-14B-Control.py)|[code](/examples/wanvideo/model_training/lora/Wan2.1-Fun-14B-Control.sh)|[code](/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-14B-Control.py)|
|[PAI/Wan2.1-Fun-V1.1-1.3B-Control](https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-1.3B-Control)|`control_video`, `reference_image`|[code](/examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-1.3B-Control.py)|[code](/examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-1.3B-Control.sh)|[code](/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-1.3B-Control.py)|[code](/examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-1.3B-Control.sh)|[code](/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-V1.1-1.3B-Control.py)|
|[PAI/Wan2.1-Fun-V1.1-14B-Control](https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-14B-Control)|`control_video`, `reference_image`|[code](/examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-14B-Control.py)|[code](/examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-14B-Control.sh)|[code](/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-14B-Control.py)|[code](/examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-14B-Control.sh)|[code](/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-V1.1-14B-Control.py)|
|[PAI/Wan2.1-Fun-V1.1-1.3B-InP](https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-1.3B-InP)|`input_image`, `end_image`|[code](/examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-1.3B-InP.py)|[code](/examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-1.3B-InP.sh)|[code](/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-1.3B-InP.py)|[code](/examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-1.3B-InP.sh)|[code](/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-V1.1-1.3B-InP.py)|
|[PAI/Wan2.1-Fun-V1.1-14B-InP](https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-14B-InP)|`input_image`, `end_image`|[code](/examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-14B-InP.py)|[code](/examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-14B-InP.sh)|[code](/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-14B-InP.py)|[code](/examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-14B-InP.sh)|[code](/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-V1.1-14B-InP.py)|
|[PAI/Wan2.1-Fun-V1.1-1.3B-Control-Camera](https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-1.3B-Control-Camera)|`control_camera_video`, `input_image`|[code](/examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-1.3B-Control-Camera.py)|[code](/examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-1.3B-Control-Camera.sh)|[code](/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-1.3B-Control-Camera.py)|[code](/examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-1.3B-Control-Camera.sh)|[code](/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-V1.1-1.3B-Control-Camera.py)|
|[PAI/Wan2.1-Fun-V1.1-14B-Control-Camera](https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-14B-Control-Camera)|`control_camera_video`, `input_image`|[code](/examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-14B-Control-Camera.py)|[code](/examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-14B-Control-Camera.sh)|[code](/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-14B-Control-Camera.py)|[code](/examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-14B-Control-Camera.sh)|[code](/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-V1.1-14B-Control-Camera.py)|
|[DiffSynth-Studio/Wan2.1-1.3b-speedcontrol-v1](https://modelscope.cn/models/DiffSynth-Studio/Wan2.1-1.3b-speedcontrol-v1)|`motion_bucket_id`|[code](/examples/wanvideo/model_inference/Wan2.1-1.3b-speedcontrol-v1.py)|[code](/examples/wanvideo/model_training/full/Wan2.1-1.3b-speedcontrol-v1.sh)|[code](/examples/wanvideo/model_training/validate_full/Wan2.1-1.3b-speedcontrol-v1.py)|[code](/examples/wanvideo/model_training/lora/Wan2.1-1.3b-speedcontrol-v1.sh)|[code](/examples/wanvideo/model_training/validate_lora/Wan2.1-1.3b-speedcontrol-v1.py)|
|[krea/krea-realtime-video](https://www.modelscope.cn/models/krea/krea-realtime-video)||[code](/examples/wanvideo/model_inference/krea-realtime-video.py)|[code](/examples/wanvideo/model_training/full/krea-realtime-video.sh)|[code](/examples/wanvideo/model_training/validate_full/krea-realtime-video.py)|[code](/examples/wanvideo/model_training/lora/krea-realtime-video.sh)|[code](/examples/wanvideo/model_training/validate_lora/krea-realtime-video.py)|
|[meituan-longcat/LongCat-Video](https://www.modelscope.cn/models/meituan-longcat/LongCat-Video)|`longcat_video`|[code](/examples/wanvideo/model_inference/LongCat-Video.py)|[code](/examples/wanvideo/model_training/full/LongCat-Video.sh)|[code](/examples/wanvideo/model_training/validate_full/LongCat-Video.py)|[code](/examples/wanvideo/model_training/lora/LongCat-Video.sh)|[code](/examples/wanvideo/model_training/validate_lora/LongCat-Video.py)|
|[ByteDance/Video-As-Prompt-Wan2.1-14B](https://modelscope.cn/models/ByteDance/Video-As-Prompt-Wan2.1-14B)|`vap_video`, `vap_prompt`|[code](/examples/wanvideo/model_inference/Video-As-Prompt-Wan2.1-14B.py)|[code](/examples/wanvideo/model_training/full/Video-As-Prompt-Wan2.1-14B.sh)|[code](/examples/wanvideo/model_training/validate_full/Video-As-Prompt-Wan2.1-14B.py)|[code](/examples/wanvideo/model_training/lora/Video-As-Prompt-Wan2.1-14B.sh)|[code](/examples/wanvideo/model_training/validate_lora/Video-As-Prompt-Wan2.1-14B.py)|
|[Wan-AI/Wan2.2-T2V-A14B](https://modelscope.cn/models/Wan-AI/Wan2.2-T2V-A14B)||[code](/examples/wanvideo/model_inference/Wan2.2-T2V-A14B.py)|[code](/examples/wanvideo/model_training/full/Wan2.2-T2V-A14B.sh)|[code](/examples/wanvideo/model_training/validate_full/Wan2.2-T2V-A14B.py)|[code](/examples/wanvideo/model_training/lora/Wan2.2-T2V-A14B.sh)|[code](/examples/wanvideo/model_training/validate_lora/Wan2.2-T2V-A14B.py)|
|[Wan-AI/Wan2.2-I2V-A14B](https://modelscope.cn/models/Wan-AI/Wan2.2-I2V-A14B)|`input_image`|[code](/examples/wanvideo/model_inference/Wan2.2-I2V-A14B.py)|[code](/examples/wanvideo/model_training/full/Wan2.2-I2V-A14B.sh)|[code](/examples/wanvideo/model_training/validate_full/Wan2.2-I2V-A14B.py)|[code](/examples/wanvideo/model_training/lora/Wan2.2-I2V-A14B.sh)|[code](/examples/wanvideo/model_training/validate_lora/Wan2.2-I2V-A14B.py)|
|[Wan-AI/Wan2.2-TI2V-5B](https://modelscope.cn/models/Wan-AI/Wan2.2-TI2V-5B)|`input_image`|[code](/examples/wanvideo/model_inference/Wan2.2-TI2V-5B.py)|[code](/examples/wanvideo/model_training/full/Wan2.2-TI2V-5B.sh)|[code](/examples/wanvideo/model_training/validate_full/Wan2.2-TI2V-5B.py)|[code](/examples/wanvideo/model_training/lora/Wan2.2-TI2V-5B.sh)|[code](/examples/wanvideo/model_training/validate_lora/Wan2.2-TI2V-5B.py)|
|[Wan-AI/Wan2.2-Animate-14B](https://www.modelscope.cn/models/Wan-AI/Wan2.2-Animate-14B)|`input_image`, `animate_pose_video`, `animate_face_video`, `animate_inpaint_video`, `animate_mask_video`|[code](/examples/wanvideo/model_inference/Wan2.2-Animate-14B.py)|[code](/examples/wanvideo/model_training/full/Wan2.2-Animate-14B.sh)|[code](/examples/wanvideo/model_training/validate_full/Wan2.2-Animate-14B.py)|[code](/examples/wanvideo/model_training/lora/Wan2.2-Animate-14B.sh)|[code](/examples/wanvideo/model_training/validate_lora/Wan2.2-Animate-14B.py)|
|[Wan-AI/Wan2.2-S2V-14B](https://www.modelscope.cn/models/Wan-AI/Wan2.2-S2V-14B)|`input_image`, `input_audio`, `audio_sample_rate`, `s2v_pose_video`|[code](/examples/wanvideo/model_inference/Wan2.2-S2V-14B_multi_clips.py)|[code](/examples/wanvideo/model_training/full/Wan2.2-S2V-14B.sh)|[code](/examples/wanvideo/model_training/validate_full/Wan2.2-S2V-14B.py)|[code](/examples/wanvideo/model_training/lora/Wan2.2-S2V-14B.sh)|[code](/examples/wanvideo/model_training/validate_lora/Wan2.2-S2V-14B.py)|
|[PAI/Wan2.2-VACE-Fun-A14B](https://www.modelscope.cn/models/PAI/Wan2.2-VACE-Fun-A14B)|`vace_control_video`, `vace_reference_image`|[code](/examples/wanvideo/model_inference/Wan2.2-VACE-Fun-A14B.py)|[code](/examples/wanvideo/model_training/full/Wan2.2-VACE-Fun-A14B.sh)|[code](/examples/wanvideo/model_training/validate_full/Wan2.2-VACE-Fun-A14B.py)|[code](/examples/wanvideo/model_training/lora/Wan2.2-VACE-Fun-A14B.sh)|[code](/examples/wanvideo/model_training/validate_lora/Wan2.2-VACE-Fun-A14B.py)|
|[PAI/Wan2.2-Fun-A14B-InP](https://modelscope.cn/models/PAI/Wan2.2-Fun-A14B-InP)|`input_image`, `end_image`|[code](/examples/wanvideo/model_inference/Wan2.2-Fun-A14B-InP.py)|[code](/examples/wanvideo/model_training/full/Wan2.2-Fun-A14B-InP.sh)|[code](/examples/wanvideo/model_training/validate_full/Wan2.2-Fun-A14B-InP.py)|[code](/examples/wanvideo/model_training/lora/Wan2.2-Fun-A14B-InP.sh)|[code](/examples/wanvideo/model_training/validate_lora/Wan2.2-Fun-A14B-InP.py)|
|[PAI/Wan2.2-Fun-A14B-Control](https://modelscope.cn/models/PAI/Wan2.2-Fun-A14B-Control)|`control_video`, `reference_image`|[code](/examples/wanvideo/model_inference/Wan2.2-Fun-A14B-Control.py)|[code](/examples/wanvideo/model_training/full/Wan2.2-Fun-A14B-Control.sh)|[code](/examples/wanvideo/model_training/validate_full/Wan2.2-Fun-A14B-Control.py)|[code](/examples/wanvideo/model_training/lora/Wan2.2-Fun-A14B-Control.sh)|[code](/examples/wanvideo/model_training/validate_lora/Wan2.2-Fun-A14B-Control.py)|
|[PAI/Wan2.2-Fun-A14B-Control-Camera](https://modelscope.cn/models/PAI/Wan2.2-Fun-A14B-Control-Camera)|`control_camera_video`, `input_image`|[code](/examples/wanvideo/model_inference/Wan2.2-Fun-A14B-Control-Camera.py)|[code](/examples/wanvideo/model_training/full/Wan2.2-Fun-A14B-Control-Camera.sh)|[code](/examples/wanvideo/model_training/validate_full/Wan2.2-Fun-A14B-Control-Camera.py)|[code](/examples/wanvideo/model_training/lora/Wan2.2-Fun-A14B-Control-Camera.sh)|[code](/examples/wanvideo/model_training/validate_lora/Wan2.2-Fun-A14B-Control-Camera.py)|

</details>

## åˆ›æ–°æˆæœ

DiffSynth-Studio ä¸ä»…ä»…æ˜¯ä¸€ä¸ªå·¥ç¨‹åŒ–çš„æ¨¡å‹æ¡†æ¶ï¼Œæ›´æ˜¯åˆ›æ–°æˆæœçš„å­µåŒ–å™¨ã€‚

<details>

<summary>Spectral Evolution Search: ç”¨äºå¥–åŠ±å¯¹é½å›¾åƒç”Ÿæˆçš„é«˜æ•ˆæ¨ç†é˜¶æ®µç¼©æ”¾</summary>

- è®ºæ–‡ï¼š[Spectral Evolution Search: Efficient Inference-Time Scaling for Reward-Aligned Image Generation
](https://arxiv.org/abs/2602.03208)
- ä»£ç æ ·ä¾‹ï¼šcoming soon

|FLUX.1-dev|FLUX.1-dev + SES|Qwen-Image|Qwen-Image + SES|
|-|-|-|-|
|![Image](https://github.com/user-attachments/assets/5be15dc6-2805-4822-b04c-2573fc0f45f0)|![Image](https://github.com/user-attachments/assets/e71b8c20-1629-41d9-b0ff-185805c1da4e)|![Image](https://github.com/user-attachments/assets/7a73c968-133a-4545-9aa2-205533861cd4)|![Image](https://github.com/user-attachments/assets/c8390b22-14fe-48a0-a6e6-d6556d31235e)|

</details>


<details>

<summary>VIRALï¼šåŸºäºDiTæ¨¡å‹çš„ç±»æ¯”è§†è§‰ä¸Šä¸‹æ–‡æ¨ç†</summary>

- è®ºæ–‡ï¼š[VIRAL: Visual In-Context Reasoning via Analogy in Diffusion Transformers
](https://arxiv.org/abs/2602.03210)
- ä»£ç æ ·ä¾‹ï¼š[/examples/qwen_image/model_inference/Qwen-Image-Edit-2511-ICEdit.py](/examples/qwen_image/model_inference/Qwen-Image-Edit-2511-ICEdit.py)
- æ¨¡å‹ï¼š[ModelScope](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Edit-2511-ICEdit-LoRA)

|Example 1|Example 2|Query|Output|
|-|-|-|-|
|![Image](https://github.com/user-attachments/assets/380d2670-47bf-41cd-b5c9-37110cc4a943)|![Image](https://github.com/user-attachments/assets/7ceaf345-0992-46e6-b38f-394c2065b165)|![Image](https://github.com/user-attachments/assets/f7c26c21-6894-4d9e-b570-f1d44ca7c1de)|![Image](https://github.com/user-attachments/assets/c2bebe3b-5984-41ba-94bf-9509f6a8a990)|

</details>


<details>

<summary>AttriCtrl: å›¾åƒç”Ÿæˆæ¨¡å‹çš„å±æ€§å¼ºåº¦æ§åˆ¶</summary>

- è®ºæ–‡ï¼š[AttriCtrl: Fine-Grained Control of Aesthetic Attribute Intensity in Diffusion Models
](https://arxiv.org/abs/2508.02151)
- ä»£ç æ ·ä¾‹ï¼š[/examples/flux/model_inference/FLUX.1-dev-AttriCtrl.py](/examples/flux/model_inference/FLUX.1-dev-AttriCtrl.py)
- æ¨¡å‹ï¼š[ModelScope](https://www.modelscope.cn/models/DiffSynth-Studio/AttriCtrl-FLUX.1-Dev)

|brightness scale = 0.1|brightness scale = 0.3|brightness scale = 0.5|brightness scale = 0.7|brightness scale = 0.9|
|-|-|-|-|-|
|![Image](https://github.com/user-attachments/assets/e74b32a5-5b2e-4c87-9df8-487c0f8366b7)|![Image](https://github.com/user-attachments/assets/bfe8bec2-9e55-493d-9a26-7e9cce28e03d)|![Image](https://github.com/user-attachments/assets/b099dfe3-ff1f-4b96-894c-d48bbe92db7a)|![Image](https://github.com/user-attachments/assets/0a6b2982-deab-4b0d-91ad-888782de01c9)|![Image](https://github.com/user-attachments/assets/fcecb755-7d03-4020-b83a-13ad2b38705c)|

</details>


<details>

<summary>AutoLoRA: è‡ªåŠ¨åŒ–çš„ LoRA æ£€ç´¢å’Œèåˆ</summary>

- è®ºæ–‡ï¼š[AutoLoRA: Automatic LoRA Retrieval and Fine-Grained Gated Fusion for Text-to-Image Generation
](https://arxiv.org/abs/2508.02107)
- ä»£ç æ ·ä¾‹ï¼š[/examples/flux/model_inference/FLUX.1-dev-LoRA-Fusion.py](/examples/flux/model_inference/FLUX.1-dev-LoRA-Fusion.py)
- æ¨¡å‹ï¼š[ModelScope](https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev)

||[LoRA 1](https://modelscope.cn/models/cancel13/cxsk)|[LoRA 2](https://modelscope.cn/models/wy413928499/xuancai2)|[LoRA 3](https://modelscope.cn/models/DiffSynth-Studio/ArtAug-lora-FLUX.1dev-v1)|[LoRA 4](https://modelscope.cn/models/hongyanbujian/JPL)|
|-|-|-|-|-|
|[LoRA 1](https://modelscope.cn/models/cancel13/cxsk)                              |![Image](https://github.com/user-attachments/assets/01c54d5a-4f00-4c2e-982a-4ec0a4c6a6e3)|![Image](https://github.com/user-attachments/assets/e6621457-b9f1-437c-bcc8-3e12e41646de)|![Image](https://github.com/user-attachments/assets/4b7f721f-a2e5-416c-af2c-b53ef236c321)|![Image](https://github.com/user-attachments/assets/802d554e-0402-482c-9f28-87605f8fe318)|
|[LoRA 2](https://modelscope.cn/models/wy413928499/xuancai2)                       |![Image](https://github.com/user-attachments/assets/e6621457-b9f1-437c-bcc8-3e12e41646de)|![Image](https://github.com/user-attachments/assets/43720a9f-aa27-4918-947d-545389375d46)|![Image](https://github.com/user-attachments/assets/418c725b-6d35-41f4-b18f-c7e3867cc142)|![Image](https://github.com/user-attachments/assets/8c8f22fa-9643-4019-b6d7-396d8b7fed9a)|
|[LoRA 3](https://modelscope.cn/models/DiffSynth-Studio/ArtAug-lora-FLUX.1dev-v1)  |![Image](https://github.com/user-attachments/assets/4b7f721f-a2e5-416c-af2c-b53ef236c321)|![Image](https://github.com/user-attachments/assets/418c725b-6d35-41f4-b18f-c7e3867cc142)|![Image](https://github.com/user-attachments/assets/041a3f9a-c7b4-4311-8582-cb71a7226d80)|![Image](https://github.com/user-attachments/assets/b54ebaa4-31a7-4536-a2c1-496adba0c013)|
|[LoRA 4](https://modelscope.cn/models/hongyanbujian/JPL)                          |![Image](https://github.com/user-attachments/assets/802d554e-0402-482c-9f28-87605f8fe318)|![Image](https://github.com/user-attachments/assets/8c8f22fa-9643-4019-b6d7-396d8b7fed9a)|![Image](https://github.com/user-attachments/assets/b54ebaa4-31a7-4536-a2c1-496adba0c013)|![Image](https://github.com/user-attachments/assets/a640fd54-3192-49a0-9281-b43d9ba64f09)|

</details>


<details>

<summary>Nexus-Gen: ç»Ÿä¸€æ¶æ„çš„å›¾åƒç†è§£ã€ç”Ÿæˆã€ç¼–è¾‘</summary>

- è¯¦ç»†é¡µé¢ï¼šhttps://github.com/modelscope/Nexus-Gen
- è®ºæ–‡ï¼š[Nexus-Gen: Unified Image Understanding, Generation, and Editing via Prefilled Autoregression in Shared Embedding Space](https://arxiv.org/pdf/2504.21356)
- æ¨¡å‹ï¼š[ModelScope](https://www.modelscope.cn/models/DiffSynth-Studio/Nexus-GenV2), [HuggingFace](https://huggingface.co/modelscope/Nexus-GenV2)
- æ•°æ®é›†ï¼š[ModelScope Dataset](https://www.modelscope.cn/datasets/DiffSynth-Studio/Nexus-Gen-Training-Dataset)
- åœ¨çº¿ä½“éªŒï¼š[ModelScope Nexus-Gen Studio](https://www.modelscope.cn/studios/DiffSynth-Studio/Nexus-Gen)

![](https://github.com/modelscope/Nexus-Gen/raw/main/assets/illustrations/gen_edit.jpg)

</details>


<details>

<summary>ArtAug: å›¾åƒç”Ÿæˆæ¨¡å‹çš„ç¾å­¦æå‡</summary>

- è¯¦ç»†é¡µé¢ï¼š[./examples/ArtAug/](./examples/ArtAug/)
- è®ºæ–‡ï¼š[ArtAug: Enhancing Text-to-Image Generation through Synthesis-Understanding Interaction](https://arxiv.org/abs/2412.12888)
- æ¨¡å‹ï¼š[ModelScope](https://www.modelscope.cn/models/DiffSynth-Studio/ArtAug-lora-FLUX.1dev-v1), [HuggingFace](https://huggingface.co/ECNU-CILab/ArtAug-lora-FLUX.1dev-v1)
- åœ¨çº¿ä½“éªŒï¼š[ModelScope AIGC Tab](https://www.modelscope.cn/aigc/imageGeneration?tab=advanced&versionId=7228&modelType=LoRA&sdVersion=FLUX_1&modelUrl=modelscope%3A%2F%2FDiffSynth-Studio%2FArtAug-lora-FLUX.1dev-v1%3Frevision%3Dv1.0)

|FLUX.1-dev|FLUX.1-dev + ArtAug LoRA|
|-|-|
|![image_1_base](https://github.com/user-attachments/assets/e1d5c505-b423-45fe-be01-25c2758f5417)|![image_1_enhance](https://github.com/user-attachments/assets/335908e3-d0bd-41c2-9d99-d10528a2d719)|

</details>


<details>

<summary>EliGen: ç²¾å‡†çš„å›¾åƒåˆ†åŒºæ§åˆ¶</summary>

- è®ºæ–‡ï¼š[EliGen: Entity-Level Controlled Image Generation with Regional Attention](https://arxiv.org/abs/2501.01097)
- ä»£ç æ ·ä¾‹ï¼š[/examples/flux/model_inference/FLUX.1-dev-EliGen.py](/examples/flux/model_inference/FLUX.1-dev-EliGen.py)
- æ¨¡å‹ï¼š[ModelScope](https://www.modelscope.cn/models/DiffSynth-Studio/Eligen), [HuggingFace](https://huggingface.co/modelscope/EliGen)
- åœ¨çº¿ä½“éªŒï¼š[ModelScope EliGen Studio](https://www.modelscope.cn/studios/DiffSynth-Studio/EliGen)
- æ•°æ®é›†ï¼š[EliGen Train Set](https://www.modelscope.cn/datasets/DiffSynth-Studio/EliGenTrainSet)

|å®ä½“æ§åˆ¶åŒºåŸŸ|ç”Ÿæˆå›¾åƒ|
|-|-|
|![eligen_example_2_mask_0](https://github.com/user-attachments/assets/1c6d9445-5022-4d91-ad2e-dc05321883d1)|![eligen_example_2_0](https://github.com/user-attachments/assets/86739945-cb07-4a49-b3b3-3bb65c90d14f)|

</details>


<details>

<summary>ExVideo: è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„æ‰©å±•è®­ç»ƒ</summary>

- é¡¹ç›®é¡µé¢ï¼š[Project Page](https://ecnu-cilab.github.io/ExVideoProjectPage/)
- è®ºæ–‡ï¼š[ExVideo: Extending Video Diffusion Models via Parameter-Efficient Post-Tuning](https://arxiv.org/abs/2406.14130)
- ä»£ç æ ·ä¾‹ï¼šè¯·å‰å¾€[æ—§ç‰ˆæœ¬](https://github.com/modelscope/DiffSynth-Studio/tree/afd101f3452c9ecae0c87b79adfa2e22d65ffdc3/examples/ExVideo)æŸ¥çœ‹
- æ¨¡å‹ï¼š[ModelScope](https://modelscope.cn/models/ECNU-CILab/ExVideo-SVD-128f-v1), [HuggingFace](https://huggingface.co/ECNU-CILab/ExVideo-SVD-128f-v1)

https://github.com/modelscope/DiffSynth-Studio/assets/35051019/d97f6aa9-8064-4b5b-9d49-ed6001bb9acc

</details>


<details>

<summary>Diffutoon: é«˜åˆ†è¾¨ç‡åŠ¨æ¼«é£æ ¼è§†é¢‘æ¸²æŸ“</summary>

- é¡¹ç›®é¡µé¢ï¼š[Project Page](https://ecnu-cilab.github.io/DiffutoonProjectPage/)
- è®ºæ–‡ï¼š[Diffutoon: High-Resolution Editable Toon Shading via Diffusion Models](https://arxiv.org/abs/2401.16224)
- ä»£ç æ ·ä¾‹ï¼šè¯·å‰å¾€[æ—§ç‰ˆæœ¬](https://github.com/modelscope/DiffSynth-Studio/tree/afd101f3452c9ecae0c87b79adfa2e22d65ffdc3/examples/Diffutoon)æŸ¥çœ‹

https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/b54c05c5-d747-4709-be5e-b39af82404dd

</details>


<details>

<summary>DiffSynth: æœ¬é¡¹ç›®çš„åˆä»£ç‰ˆæœ¬</summary>

- é¡¹ç›®é¡µé¢ï¼š[Project Page](https://ecnu-cilab.github.io/DiffSynth.github.io/)
- è®ºæ–‡ï¼š[DiffSynth: Latent In-Iteration Deflickering for Realistic Video Synthesis](https://arxiv.org/abs/2308.03463)
- ä»£ç æ ·ä¾‹ï¼šè¯·å‰å¾€[æ—§ç‰ˆæœ¬](https://github.com/modelscope/DiffSynth-Studio/tree/afd101f3452c9ecae0c87b79adfa2e22d65ffdc3/examples/diffsynth)æŸ¥çœ‹

https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/59fb2f7b-8de0-4481-b79f-0c3a7361a1ea

</details>
